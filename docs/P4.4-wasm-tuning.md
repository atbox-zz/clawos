# P4.4: WASM Memory/CPU Limit Fine-Tuning Strategy

**Status**: DRAFT
**Version**: 0.1.0
**Created**: 2026-02-24
**Owner**: Infra Agent
**Related Specs**: P1.5 (cgroup quotas)

---

## Executive Summary

This document defines the fine-tuning strategy for WASM sandbox resource limits in ClawOS. The strategy balances security isolation with performance optimization through adaptive scaling based on workload characteristics and system conditions.

**Base Limits (from P1.5)**:
- Memory: 256M per WASM instance
- CPU: 5% quota per WASM instance
- Max instances: 16 (4G total memory, 80% total CPU)

---

## 1. Memory Limit Tuning

### 1.1 Base Configuration

```yaml
memory:
  base_limit: 256M
  min_limit: 128M
  max_limit: 512M
  swap_enabled: false
  oom_group: true
```

### 1.2 Memory Tier Strategy

| Tier | Limit | Use Case | Characteristics |
|------|-------|----------|-----------------|
| **T0** | 128M | Simple tools (grep, sed, awk) | Minimal memory, fast startup |
| **T1** | 256M | Standard tools (file processing, JSON parsing) | Default tier, balanced |
| **T2** | 384M | Heavy tools (data transformation, compression) | For memory-intensive operations |
| **T3** | 512M | Specialized tools (ML inference, large datasets) | Exceptional use, requires approval |

### 1.3 Memory Allocation Algorithm

```python
def determine_memory_tier(tool_metadata, historical_data):
    """
    Determine appropriate memory tier based on tool characteristics
    """
    base_score = 0

    # Factor 1: Tool category (0-40 points)
    category_scores = {
        'text_processing': 10,
        'data_transform': 20,
        'compression': 30,
        'ml_inference': 40,
        'network_proxy': 15
    }
    base_score += category_scores.get(tool_metadata.category, 10)

    # Factor 2: Historical peak memory (0-30 points)
    if historical_data:
        peak_mb = historical_data.peak_memory_mb
        if peak_mb < 64:
            base_score += 0
        elif peak_mb < 128:
            base_score += 10
        elif peak_mb < 256:
            base_score += 20
        else:
            base_score += 30

    # Factor 3: Input size estimate (0-20 points)
    input_size_mb = estimate_input_size(tool_metadata)
    if input_size_mb < 10:
        base_score += 0
    elif input_size_mb < 50:
        base_score += 10
    elif input_size_mb < 100:
        base_score += 15
    else:
        base_score += 20

    # Factor 4: Tool complexity (0-10 points)
    base_score += min(tool_metadata.complexity_score, 10)

    # Map score to tier
    if base_score < 20:
        return 'T0', '128M'
    elif base_score < 40:
        return 'T1', '256M'
    elif base_score < 60:
        return 'T2', '384M'
    else:
        return 'T3', '512M'
```

### 1.4 Dynamic Memory Adjustment

**Trigger Conditions**:
- Memory usage > 90% of limit for 3 consecutive samples (15 seconds)
- OOM event detected in last 5 minutes
- Historical data shows consistent underutilization (< 30%)

**Adjustment Rules**:
```yaml
memory_adjustment:
  scale_up:
    trigger: "usage > 90% for 15s OR oom_event"
    factor: 1.5
    max_tier: T3
    cooldown: 60s

  scale_down:
    trigger: "usage < 30% for 5min AND no oom_events in 10min"
    factor: 0.75
    min_tier: T0
    cooldown: 300s
```

### 1.5 Memory Pressure Detection

**Metrics to Monitor**:
```bash
# cgroup v2 memory metrics
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.current
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.stat
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.oom_events
```

**Pressure Levels**:
| Level | Threshold | Action |
|-------|-----------|--------|
| **LOW** | < 50% | No action, continue monitoring |
| **MEDIUM** | 50-75% | Log warning, prepare scale-up |
| **HIGH** | 75-90% | Trigger scale-up evaluation |
| **CRITICAL** | > 90% | Immediate scale-up or graceful degradation |

---

## 2. CPU Quota Tuning

### 2.1 Base Configuration

```yaml
cpu:
  base_quota: 50000  # 5% of 1 second (1000000us)
  period: 1000000    # 1 second
  min_quota: 25000   # 2.5%
  max_quota: 150000  # 15%
  weight: 5000       # Relative priority
```

### 2.2 CPU Tier Strategy

| Tier | Quota | Use Case | Characteristics |
|------|-------|----------|-----------------|
| **C0** | 2.5% | Idle/low-frequency tools | Minimal CPU, background tasks |
| **C1** | 5% | Standard tools | Default tier |
| **C2** | 7.5% | CPU-intensive tools | For computation-heavy operations |
| **C3** | 10% | High-performance tools | Time-critical operations |
| **C4** | 15% | Exceptional tools | Requires approval |

### 2.3 CPU Allocation Algorithm

```python
def determine_cpu_tier(tool_metadata, historical_data):
    """
    Determine appropriate CPU tier based on tool characteristics
    """
    base_score = 0

    # Factor 1: Tool category (0-40 points)
    category_scores = {
        'text_processing': 10,
        'data_transform': 20,
        'compression': 30,
        'ml_inference': 35,
        'network_proxy': 15,
        'crypto': 40
    }
    base_score += category_scores.get(tool_metadata.category, 10)

    # Factor 2: Historical CPU utilization (0-30 points)
    if historical_data:
        avg_cpu_pct = historical_data.avg_cpu_percent
        if avg_cpu_pct < 20:
            base_score += 0
        elif avg_cpu_pct < 40:
            base_score += 10
        elif avg_cpu_pct < 60:
            base_score += 20
        else:
            base_score += 30

    # Factor 3: Latency requirements (0-20 points)
    if tool_metadata.latency_sensitivity == 'high':
        base_score += 20
    elif tool_metadata.latency_sensitivity == 'medium':
        base_score += 10

    # Factor 4: Parallelism potential (0-10 points)
    base_score += min(tool_metadata.parallelism_score, 10)

    # Map score to tier
    if base_score < 20:
        return 'C0', '25000'
    elif base_score < 40:
        return 'C1', '50000'
    elif base_score < 60:
        return 'C2', '75000'
    elif base_score < 80:
        return 'C3', '100000'
    else:
        return 'C4', '150000'
```

### 2.4 Dynamic CPU Adjustment

**Trigger Conditions**:
- CPU throttling detected (cpu.stat.throttled_time > 0)
- CPU usage > 80% of quota for 10 consecutive samples (50 seconds)
- Latency SLA violations detected
- Historical data shows consistent underutilization (< 20%)

**Adjustment Rules**:
```yaml
cpu_adjustment:
  scale_up:
    trigger: "throttling OR usage > 80% for 50s OR sla_violation"
    factor: 1.5
    max_tier: C4
    cooldown: 30s

  scale_down:
    trigger: "usage < 20% for 5min AND no sla_violations in 10min"
    factor: 0.75
    min_tier: C0
    cooldown: 300s
```

### 2.5 CPU Throttling Detection

**Metrics to Monitor**:
```bash
# cgroup v2 CPU metrics
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/cpu.stat
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/cpu.pressure
```

**Key Metrics**:
- `nr_periods`: Total scheduling periods
- `nr_throttled`: Periods where quota was exhausted
- `throttled_time`: Total time throttled (nanoseconds)
- `throttled_pct`: (nr_throttled / nr_periods) * 100

**Throttling Levels**:
| Level | Throttled % | Action |
|-------|-------------|--------|
| **NONE** | 0% | No action |
| **LOW** | < 5% | Log warning, monitor |
| **MEDIUM** | 5-15% | Evaluate scale-up |
| **HIGH** | > 15% | Immediate scale-up |

---

## 3. Benchmark Procedures for WASM Workloads

### 3.1 Benchmark Categories

| Category | Description | Representative Tools |
|----------|-------------|---------------------|
| **Text Processing** | String manipulation, regex, parsing | grep, sed, awk, jq |
| **Data Transform** | JSON/YAML conversion, CSV processing | jq, yq, csvtool |
| **Compression** | Archive creation/extraction | gzip, bzip2, xz |
| **Network Proxy** | HTTP request handling | curl, wget |
| **Crypto** | Hashing, encryption | sha256sum, openssl |
| **ML Inference** | Model execution | ONNX Runtime, TensorFlow Lite |

### 3.2 Benchmark Methodology

#### 3.2.1 Memory Benchmark

```bash
#!/bin/bash
# benchmark_memory.sh - Memory usage profiling for WASM tools

TOOL_NAME=$1
INPUT_SIZE=$2
ITERATIONS=10

echo "Benchmarking memory usage for: $TOOL_NAME"
echo "Input size: $INPUT_SIZE"
echo "Iterations: $ITERATIONS"

# Create cgroup for benchmark
CGROUP_PATH="/sys/fs/cgroup/clawos/benchmarks/${TOOL_NAME}"
mkdir -p "$CGROUP_PATH"

# Set memory limit (start with 256M)
echo "256M" > "$CGROUP_PATH/memory.max"
echo "0" > "$CGROUP_PATH/memory.swap.max"

# Run benchmark
for i in $(seq 1 $ITERATIONS); do
    # Reset memory stats
    echo 0 > "$CGROUP_PATH/memory.oom_events"

    # Run tool in cgroup
    cgexec -g "memory:${CGROUP_PATH}" \
        $TOOL_NAME $INPUT_SIZE > /dev/null 2>&1

    # Capture metrics
    MEMORY_CURRENT=$(cat "$CGROUP_PATH/memory.current")
    MEMORY_PEAK=$(cat "$CGROUP_PATH/memory.peak")
    OOM_EVENTS=$(cat "$CGROUP_PATH/memory.oom_events")

    echo "Iteration $i: current=$MEMORY_CURRENT, peak=$MEMORY_PEAK, oom=$OOM_EVENTS"
done

# Cleanup
rmdir "$CGROUP_PATH"
```

#### 3.2.2 CPU Benchmark

```bash
#!/bin/bash
# benchmark_cpu.sh - CPU usage profiling for WASM tools

TOOL_NAME=$1
INPUT_SIZE=$2
ITERATIONS=10

echo "Benchmarking CPU usage for: $TOOL_NAME"
echo "Input size: $INPUT_SIZE"
echo "Iterations: $ITERATIONS"

# Create cgroup for benchmark
CGROUP_PATH="/sys/fs/cgroup/clawos/benchmarks/${TOOL_NAME}"
mkdir -p "$CGROUP_PATH"

# Set CPU quota (start with 5%)
echo "50000 1000000" > "$CGROUP_PATH/cpu.max"

# Run benchmark
for i in $(seq 1 $ITERATIONS); do
    # Reset CPU stats
    echo 0 > "$CGROUP_PATH/cpu.stat"

    # Run tool in cgroup
    START_TIME=$(date +%s%N)
    cgexec -g "cpu:${CGROUP_PATH}" \
        $TOOL_NAME $INPUT_SIZE > /dev/null 2>&1
    END_TIME=$(date +%s%N)

    # Capture metrics
    DURATION_NS=$((END_TIME - START_TIME))
    DURATION_MS=$((DURATION_NS / 1000000))

    NR_PERIODS=$(cat "$CGROUP_PATH/cpu.stat" | grep nr_periods | awk '{print $2}')
    NR_THROTTLED=$(cat "$CGROUP_PATH/cpu.stat" | grep nr_throttled | awk '{print $2}')
    THROTTLED_TIME=$(cat "$CGROUP_PATH/cpu.stat" | grep throttled_time | awk '{print $2}')

    THROTTLED_PCT=$(echo "scale=2; $NR_THROTTLED * 100 / $NR_PERIODS" | bc)

    echo "Iteration $i: duration=${DURATION_MS}ms, throttled=${THROTTLED_PCT}%"
done

# Cleanup
rmdir "$CGROUP_PATH"
```

#### 3.2.3 Combined Benchmark

```bash
#!/bin/bash
# benchmark_combined.sh - Full resource profiling

TOOL_NAME=$1
INPUT_SIZES=("1K" "10K" "100K" "1M" "10M")

for SIZE in "${INPUT_SIZES[@]}"; do
    echo "========================================"
    echo "Benchmark: $TOOL_NAME with input size $SIZE"
    echo "========================================"

    # Memory benchmark
    ./benchmark_memory.sh $TOOL_NAME $SIZE

    # CPU benchmark
    ./benchmark_cpu.sh $TOOL_NAME $SIZE

    echo ""
done
```

### 3.3 Benchmark Data Collection

**Metrics to Record**:
```yaml
benchmark_metrics:
  memory:
    - current_usage_mb
    - peak_usage_mb
    - oom_events
    - page_faults
    - swap_usage

  cpu:
    - duration_ms
    - nr_periods
    - nr_throttled
    - throttled_time_ns
    - throttled_pct
    - cpu_cycles

  system:
    - timestamp
    - tool_name
    - input_size
    - iteration
    - cgroup_path
```

### 3.4 Benchmark Analysis

**Tier Assignment Criteria**:
```python
def analyze_benchmark_results(results):
    """
    Analyze benchmark results and assign resource tiers
    """
    memory_tiers = []
    cpu_tiers = []

    for result in results:
        # Memory tier based on peak usage
        peak_mb = result.peak_usage_mb
        if peak_mb < 64:
            memory_tiers.append('T0')
        elif peak_mb < 128:
            memory_tiers.append('T1')
        elif peak_mb < 256:
            memory_tiers.append('T2')
        else:
            memory_tiers.append('T3')

        # CPU tier based on throttling
        throttled_pct = result.throttled_pct
        if throttled_pct < 5:
            cpu_tiers.append('C0')
        elif throttled_pct < 15:
            cpu_tiers.append('C1')
        elif throttled_pct < 30:
            cpu_tiers.append('C2')
        else:
            cpu_tiers.append('C3')

    # Return most common tiers (mode)
    return {
        'memory_tier': max(set(memory_tiers), key=memory_tiers.count),
        'cpu_tier': max(set(cpu_tiers), key=cpu_tiers.count),
        'confidence': calculate_confidence(results)
    }
```

---

## 4. OOM Handling Strategies

### 4.1 OOM Prevention

**Pre-OOM Detection**:
```python
def detect_oom_risk(cgroup_path):
    """
    Detect OOM risk before actual OOM occurs
    """
    memory_current = read_cgroup_metric(cgroup_path, 'memory.current')
    memory_max = read_cgroup_metric(cgroup_path, 'memory.max')

    usage_pct = (memory_current / memory_max) * 100

    # Check memory pressure
    pressure_file = f"{cgroup_path}/memory.pressure"
    if os.path.exists(pressure_file):
        with open(pressure_file, 'r') as f:
            pressure_data = f.read()
            # Parse pressure metrics (some, full)
            # If 'full' pressure > 10%, high OOM risk

    if usage_pct > 90:
        return 'CRITICAL'
    elif usage_pct > 75:
        return 'HIGH'
    elif usage_pct > 50:
        return 'MEDIUM'
    else:
        return 'LOW'
```

### 4.2 OOM Response Strategy

```yaml
oom_response:
  prevention:
    - monitor memory.current at 1s intervals
    - trigger scale-up at 75% usage
    - log warnings at 50% usage

  mitigation:
    - graceful degradation: reduce input size
    - request streaming: process in chunks
    - cache eviction: free unused memory

  recovery:
    - capture OOM event details
    - log stack trace if available
    - restart with increased memory limit
    - update historical data for future tuning
```

### 4.3 OOM Event Handling

**Event Flow**:
```
1. OOM Event Detected
   ↓
2. Capture Context
   - Tool name and version
   - Input size and type
   - Memory limit at time of OOM
   - Current memory usage
   - Stack trace (if available)
   ↓
3. Log Event
   - Structured log entry
   - Send to monitoring system
   - Update historical database
   ↓
4. Immediate Action
   - Kill WASM instance (graceful if possible)
   - Prevent cascading failures (oom.group=1)
   ↓
5. Recovery
   - Analyze root cause
   - Adjust memory tier for future runs
   - Notify operator if recurring
```

### 4.4 OOM Event Schema

```json
{
  "event_type": "oom",
  "timestamp": "2026-02-24T10:30:00Z",
  "instance_id": "wasm-12345",
  "tool_name": "jq",
  "tool_version": "1.6",
  "memory_limit_mb": 256,
  "memory_usage_mb": 255,
  "input_size_mb": 50,
  "input_type": "json",
  "stack_trace": "...",
  "recovery_action": "scaled_to_384M",
  "recurrence_count": 1
}
```

---

## 5. Performance Degradation Detection

### 5.1 Degradation Metrics

**Key Indicators**:
```yaml
degradation_metrics:
  latency:
    - p50_latency_ms
    - p95_latency_ms
    - p99_latency_ms
    - sla_violation_count

  throughput:
    - requests_per_second
    - success_rate
    - error_rate

  resource:
    - cpu_throttling_pct
    - memory_pressure_pct
    - io_wait_pct

  quality:
    - result_accuracy
    - timeout_count
    - retry_count
```

### 5.2 Degradation Detection Algorithm

```python
def detect_performance_degradation(current_metrics, baseline_metrics, window=5):
    """
    Detect performance degradation using statistical analysis
    """
    degradation_signals = []

    # Latency degradation
    latency_increase = (
        current_metrics.p95_latency_ms / baseline_metrics.p95_latency_ms
    )
    if latency_increase > 2.0:  # 2x slower
        degradation_signals.append({
            'type': 'latency',
            'severity': 'HIGH',
            'value': latency_increase,
            'threshold': 2.0
        })
    elif latency_increase > 1.5:  # 1.5x slower
        degradation_signals.append({
            'type': 'latency',
            'severity': 'MEDIUM',
            'value': latency_increase,
            'threshold': 1.5
        })

    # Throughput degradation
    throughput_decrease = (
        baseline_metrics.requests_per_second / current_metrics.requests_per_second
    )
    if throughput_decrease > 2.0:  # 2x less throughput
        degradation_signals.append({
            'type': 'throughput',
            'severity': 'HIGH',
            'value': throughput_decrease,
            'threshold': 2.0
        })

    # CPU throttling
    if current_metrics.cpu_throttling_pct > 15:
        degradation_signals.append({
            'type': 'cpu_throttling',
            'severity': 'HIGH',
            'value': current_metrics.cpu_throttling_pct,
            'threshold': 15
        })

    # Memory pressure
    if current_metrics.memory_pressure_pct > 75:
        degradation_signals.append({
            'type': 'memory_pressure',
            'severity': 'HIGH',
            'value': current_metrics.memory_pressure_pct,
            'threshold': 75
        })

    # Determine overall degradation level
    if any(s['severity'] == 'HIGH' for s in degradation_signals):
        return 'CRITICAL', degradation_signals
    elif any(s['severity'] == 'MEDIUM' for s in degradation_signals):
        return 'WARNING', degradation_signals
    else:
        return 'NORMAL', []
```

### 5.3 Degradation Response

```yaml
degradation_response:
  CRITICAL:
    - immediate scale-up (memory and/or CPU)
    - alert operator
    - consider instance restart
    - log detailed diagnostics

  WARNING:
    - evaluate scale-up
    - log warning
    - increase monitoring frequency
    - prepare mitigation plan

  NORMAL:
    - continue normal operation
    - maintain baseline monitoring
```

### 5.4 Baseline Establishment

**Baseline Collection**:
```python
def establish_baseline(tool_name, sample_size=100):
    """
    Establish performance baseline for a tool
    """
    metrics = []

    for i in range(sample_size):
        # Run tool with standard input
        result = run_tool(tool_name, standard_input)

        metrics.append({
            'latency_ms': result.duration_ms,
            'memory_mb': result.memory_usage_mb,
            'cpu_pct': result.cpu_usage_pct,
            'success': result.success
        })

    # Calculate statistics
    baseline = {
        'p50_latency_ms': percentile(metrics, 'latency_ms', 50),
        'p95_latency_ms': percentile(metrics, 'latency_ms', 95),
        'p99_latency_ms': percentile(metrics, 'latency_ms', 99),
        'avg_memory_mb': mean(metrics, 'memory_mb'),
        'p95_memory_mb': percentile(metrics, 'memory_mb', 95),
        'avg_cpu_pct': mean(metrics, 'cpu_pct'),
        'success_rate': sum(m['success'] for m in metrics) / len(metrics)
    }

    return baseline
```

---

## 6. Adaptive Scaling Thresholds

### 6.1 Scaling Decision Matrix

| Condition | Memory Action | CPU Action | Priority |
|-----------|---------------|------------|----------|
| OOM event | Scale up 2x | No change | P0 |
| Memory > 90% | Scale up 1.5x | No change | P1 |
| CPU throttling > 15% | No change | Scale up 1.5x | P1 |
| Memory < 30% (5min) | Scale down 0.75x | No change | P2 |
| CPU < 20% (5min) | No change | Scale down 0.75x | P2 |
| SLA violation | Scale up 1.5x | Scale up 1.5x | P0 |
| Degradation detected | Evaluate | Evaluate | P1 |

### 6.2 Scaling Cooldown Periods

```yaml
scaling_cooldowns:
  scale_up:
    memory: 60s
    cpu: 30s
    both: 30s

  scale_down:
    memory: 300s
    cpu: 300s
    both: 300s
```

### 6.3 Adaptive Scaling Algorithm

```python
class AdaptiveScaler:
    def __init__(self, instance_id):
        self.instance_id = instance_id
        self.current_memory_tier = 'T1'  # Default: 256M
        self.current_cpu_tier = 'C1'     # Default: 5%
        self.last_scale_up_time = 0
        self.last_scale_down_time = 0

    def evaluate_scaling(self, metrics):
        """
        Evaluate if scaling is needed based on current metrics
        """
        actions = []
        current_time = time.time()

        # Check scale-up conditions
        if metrics.oom_event:
            actions.append(('memory', 'scale_up', 2.0))
            actions.append(('cpu', 'scale_up', 1.5))

        elif metrics.memory_usage_pct > 90:
            if current_time - self.last_scale_up_time > 60:
                actions.append(('memory', 'scale_up', 1.5))

        elif metrics.cpu_throttling_pct > 15:
            if current_time - self.last_scale_up_time > 30:
                actions.append(('cpu', 'scale_up', 1.5))

        # Check scale-down conditions
        elif metrics.memory_usage_pct < 30:
            if current_time - self.last_scale_down_time > 300:
                if self.no_oom_events_in_last(600):
                    actions.append(('memory', 'scale_down', 0.75))

        elif metrics.cpu_usage_pct < 20:
            if current_time - self.last_scale_down_time > 300:
                if self.no_sla_violations_in_last(600):
                    actions.append(('cpu', 'scale_down', 0.75))

        return actions

    def apply_scaling(self, actions):
        """
        Apply scaling actions
        """
        for resource, action, factor in actions:
            if resource == 'memory':
                self._scale_memory(action, factor)
            elif resource == 'cpu':
                self._scale_cpu(action, factor)

    def _scale_memory(self, action, factor):
        """
        Scale memory limit
        """
        current_limit = MEMORY_TIERS[self.current_memory_tier]

        if action == 'scale_up':
            new_limit = int(current_limit * factor)
            new_tier = self._find_memory_tier(new_limit)
            if new_tier != self.current_memory_tier:
                self._apply_memory_limit(new_limit)
                self.current_memory_tier = new_tier
                self.last_scale_up_time = time.time()

        elif action == 'scale_down':
            new_limit = int(current_limit * factor)
            new_tier = self._find_memory_tier(new_limit)
            if new_tier != self.current_memory_tier:
                self._apply_memory_limit(new_limit)
                self.current_memory_tier = new_tier
                self.last_scale_down_time = time.time()

    def _scale_cpu(self, action, factor):
        """
        Scale CPU quota
        """
        current_quota = CPU_TIERS[self.current_cpu_tier]

        if action == 'scale_up':
            new_quota = int(current_quota * factor)
            new_tier = self._find_cpu_tier(new_quota)
            if new_tier != self.current_cpu_tier:
                self._apply_cpu_quota(new_quota)
                self.current_cpu_tier = new_tier
                self.last_scale_up_time = time.time()

        elif action == 'scale_down':
            new_quota = int(current_quota * factor)
            new_tier = self._find_cpu_tier(new_quota)
            if new_tier != self.current_cpu_tier:
                self._apply_cpu_quota(new_quota)
                self.current_cpu_tier = new_tier
                self.last_scale_down_time = time.time()
```

### 6.4 Global Resource Management

**Instance Pool Management**:
```python
class WASMInstancePool:
    def __init__(self, max_instances=16):
        self.max_instances = max_instances
        self.instances = {}
        self.total_memory_limit = 4 * 1024  # 4G
        self.total_cpu_quota = 80  # 80%

    def allocate_instance(self, tool_metadata):
        """
        Allocate a WASM instance with appropriate resources
        """
        # Check if pool has capacity
        if len(self.instances) >= self.max_instances:
            raise ResourceExhaustedError("Max instances reached")

        # Determine resource tiers
        memory_tier, memory_limit = determine_memory_tier(
            tool_metadata,
            self.get_historical_data(tool_metadata.name)
        )
        cpu_tier, cpu_quota = determine_cpu_tier(
            tool_metadata,
            self.get_historical_data(tool_metadata.name)
        )

        # Check global resource availability
        if not self._check_global_capacity(memory_limit, cpu_quota):
            # Try to scale down existing instances
            self._rebalance_resources()

            if not self._check_global_capacity(memory_limit, cpu_quota):
                raise ResourceExhaustedError("Insufficient global resources")

        # Create instance
        instance_id = f"wasm-{uuid.uuid4().hex[:8]}"
        self.instances[instance_id] = {
            'memory_tier': memory_tier,
            'cpu_tier': cpu_tier,
            'memory_limit': memory_limit,
            'cpu_quota': cpu_quota,
            'tool_name': tool_metadata.name,
            'created_at': time.time()
        }

        return instance_id

    def _check_global_capacity(self, memory_limit, cpu_quota):
        """
        Check if global resources are available
        """
        total_memory_used = sum(
            inst['memory_limit'] for inst in self.instances.values()
        )
        total_cpu_used = sum(
            inst['cpu_quota'] for inst in self.instances.values()
        )

        return (
            total_memory_used + memory_limit <= self.total_memory_limit and
            total_cpu_used + cpu_quota <= self.total_cpu_quota
        )

    def _rebalance_resources(self):
        """
        Rebalance resources by scaling down idle instances
        """
        for instance_id, instance in self.instances.items():
            idle_time = time.time() - instance.get('last_used_at', instance['created_at'])

            if idle_time > 300:  # 5 minutes idle
                # Scale down to minimum
                if instance['memory_tier'] != 'T0':
                    self._scale_instance(instance_id, 'memory', 'T0')
                if instance['cpu_tier'] != 'C0':
                    self._scale_instance(instance_id, 'cpu', 'C0')
```

---

## 7. Implementation Roadmap

### 7.1 Phase 1: Foundation (Week 1-2)

- [ ] Implement cgroup monitoring daemon
- [ ] Create benchmark suite
- [ ] Establish baseline metrics for common tools
- [ ] Implement OOM event logging

### 7.2 Phase 2: Adaptive Scaling (Week 3-4)

- [ ] Implement memory tier assignment algorithm
- [ ] Implement CPU tier assignment algorithm
- [ ] Create adaptive scaler component
- [ ] Implement scaling cooldown logic

### 7.3 Phase 3: Degradation Detection (Week 5-6)

- [ ] Implement performance degradation detection
- [ ] Create baseline establishment tool
- [ ] Implement degradation response logic
- [ ] Integrate with alerting system

### 7.4 Phase 4: Global Resource Management (Week 7-8)

- [ ] Implement instance pool manager
- [ ] Create global resource tracking
- [ ] Implement resource rebalancing
- [ ] Add capacity planning tools

### 7.5 Phase 5: Testing & Validation (Week 9-10)

- [ ] Load testing with realistic workloads
- [ ] Chaos testing (OOM injection, CPU throttling)
- [ ] Performance regression testing
- [ ] Security validation (resource isolation)

---

## 8. Monitoring and Observability

### 8.1 Key Metrics Dashboard

```yaml
dashboard_metrics:
  instance_level:
    - memory_usage_pct
    - cpu_usage_pct
    - cpu_throttling_pct
    - oom_events
    - latency_p50_p95_p99
    - success_rate

  pool_level:
    - active_instances
    - idle_instances
    - total_memory_used
    - total_cpu_used
    - resource_utilization_pct

  system_level:
    - total_wasm_memory
    - total_wasm_cpu
    - global_oom_events
    - scaling_events
    - degradation_events
```

### 8.2 Alerting Rules

```yaml
alerts:
  critical:
    - name: "WASM OOM Event"
      condition: "oom_events > 0"
      duration: "0s"
      action: "page_operator"

    - name: "WASM Resource Exhaustion"
      condition: "active_instances >= max_instances"
      duration: "5m"
      action: "page_operator"

  warning:
    - name: "WASM High Memory Usage"
      condition: "memory_usage_pct > 90"
      duration: "1m"
      action: "log_warning"

    - name: "WASM High CPU Throttling"
      condition: "cpu_throttling_pct > 15"
      duration: "1m"
      action: "log_warning"

    - name: "WASM Performance Degradation"
      condition: "degradation_level == CRITICAL"
      duration: "30s"
      action: "log_warning"

  info:
    - name: "WASM Scaling Event"
      condition: "scaling_event == true"
      duration: "0s"
      action: "log_info"
```

---

## 9. Security Considerations

### 9.1 Resource Isolation

- **Memory**: Each instance in dedicated cgroup with `memory.oom.group=1`
- **CPU**: CPU quota prevents CPU starvation of other components
- **Network**: No direct network egress; all traffic proxied through host
- **Filesystem**: Read-only rootfs with tmpfs for writable areas

### 9.2 Attack Mitigation

```yaml
attack_vectors:
  memory_exhaustion:
    prevention: "memory.max + memory.oom.group=1"
    detection: "monitor memory.current at 1s intervals"
    response: "kill instance, log event, prevent recurrence"

  cpu_starvation:
    prevention: "cpu.max quota enforcement"
    detection: "monitor cpu.stat throttling"
    response: "scale down or terminate abusive instance"

  resource_hoarding:
    prevention: "global resource pool management"
    detection: "monitor instance idle time"
    response: "scale down idle instances"

  side_channel:
    prevention: "separate cgroups, no shared memory"
    detection: "monitor cross-instance communication"
    response: "terminate suspicious instances"
```

---

## 10. Appendix

### 10.1 Resource Tier Reference

```yaml
memory_tiers:
  T0: 128M
  T1: 256M
  T2: 384M
  T3: 512M

cpu_tiers:
  C0: 25000   # 2.5%
  C1: 50000   # 5%
  C2: 75000   # 7.5%
  C3: 100000  # 10%
  C4: 150000  # 15%
```

### 10.2 cgroup v2 File Reference

```bash
# Memory control files
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.max
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.current
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.peak
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.stat
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.oom_events
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.pressure
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/memory.swap.max

# CPU control files
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/cpu.max
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/cpu.stat
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/cpu.pressure
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/cpu.weight

# PID control files
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/pids.max
/sys/fs/cgroup/clawos/wasm.slice/wasm-{instance}/pids.current
```

### 10.3 Benchmark Results Template

```json
{
  "tool_name": "jq",
  "tool_version": "1.6",
  "benchmark_date": "2026-02-24",
  "input_sizes": ["1K", "10K", "100K", "1M", "10M"],
  "results": {
    "1K": {
      "memory_peak_mb": 8,
      "cpu_throttling_pct": 0,
      "duration_ms": 15,
      "recommended_memory_tier": "T0",
      "recommended_cpu_tier": "C0"
    },
    "10K": {
      "memory_peak_mb": 16,
      "cpu_throttling_pct": 0,
      "duration_ms": 45,
      "recommended_memory_tier": "T0",
      "recommended_cpu_tier": "C0"
    },
    "100K": {
      "memory_peak_mb": 64,
      "cpu_throttling_pct": 2,
      "duration_ms": 320,
      "recommended_memory_tier": "T1",
      "recommended_cpu_tier": "C1"
    },
    "1M": {
      "memory_peak_mb": 192,
      "cpu_throttling_pct": 8,
      "duration_ms": 2800,
      "recommended_memory_tier": "T1",
      "recommended_cpu_tier": "C2"
    },
    "10M": {
      "memory_peak_mb": 480,
      "cpu_throttling_pct": 25,
      "duration_ms": 32000,
      "recommended_memory_tier": "T3",
      "recommended_cpu_tier": "C3"
    }
  }
}
```

---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 0.1.0 | 2026-02-24 | Initial draft |
