# P3.7: LLM Provider Bridge Configuration Specification

## Overview

The LLM Provider Bridge is a unified abstraction layer that enables ClawOS to interact with multiple LLM providers (NEAR AI, OpenRouter, and future providers) through a consistent interface. This specification defines the configuration, API integration patterns, and operational behaviors for the bridge.

## Table of Contents

1. [Architecture](#architecture)
2. [Provider Interface](#provider-interface)
3. [NEAR AI Integration](#near-ai-integration)
4. [OpenRouter Integration](#openrouter-integration)
5. [API Key Management](#api-key-management)
6. [Fallback Logic](#fallback-logic)
7. [Rate Limiting & Quotas](#rate-limiting--quotas)
8. [Configuration Schema](#configuration-schema)
9. [Example Configurations](#example-configurations)
10. [Error Handling](#error-handling)

---

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Application Layer                        │
│  (Channels, Tools, MCP Servers, Interactive Sessions)       │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│                  LLM Provider Bridge                         │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Provider Manager                                     │  │
│  │  - Provider selection                                 │  │
│  │  - Fallback orchestration                            │  │
│  │  - Health monitoring                                  │  │
│  └──────────────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Rate Limiter                                         │  │
│  │  - Request throttling                                 │  │
│  │  - Quota enforcement                                  │  │
│  │  - Token bucket management                            │  │
│  └──────────────────────────────────────────────────────┘  │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Unified Interface                                    │  │
│  │  - completion()                                       │  │
│  │  - stream()                                           │  │
│  │  - embeddings()                                       │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
         ┌───────────────┼───────────────┐
         ▼               ▼               ▼
┌────────────────┐ ┌──────────────┐ ┌──────────────┐
│   NEAR AI      │ │  OpenRouter  │ │  Future...   │
│   Provider     │ │  Provider    │ │  Providers   │
└────────────────┘ └──────────────┘ └──────────────┘
```

---

## Provider Interface

### Base Provider Contract

All LLM providers must implement the following interface:

```typescript
interface LLMProvider {
  // Provider metadata
  readonly name: string;
  readonly version: string;
  readonly capabilities: ProviderCapabilities;

  // Core operations
  completion(request: CompletionRequest): Promise<CompletionResponse>;
  stream(request: CompletionRequest): AsyncIterable<StreamChunk>;
  embeddings(request: EmbeddingsRequest): Promise<EmbeddingsResponse>;

  // Health & status
  healthCheck(): Promise<HealthStatus>;
  getQuotaStatus(): Promise<QuotaStatus>;

  // Configuration
  configure(config: ProviderConfig): void;
  getConfig(): ProviderConfig;
}
```

### Provider Capabilities

```typescript
interface ProviderCapabilities {
  // Supported operations
  completion: boolean;
  streaming: boolean;
  embeddings: boolean;

  // Model support
  models: ModelInfo[];
  defaultModel: string;

  // Feature flags
  supportsSystemPrompt: boolean;
  supportsTools: boolean;
  supportsJsonMode: boolean;
  supportsVision: boolean;

  // Limits
  maxTokens: number;
  maxContextLength: number;
  maxBatchSize: number;
}
```

### Completion Request

```typescript
interface CompletionRequest {
  // Required
  model: string;
  messages: Message[];

  // Optional
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  stopSequences?: string[];

  // Advanced
  tools?: Tool[];
  toolChoice?: 'auto' | 'required' | 'none' | { type: 'function', function: { name: string } };
  responseFormat?: { type: 'text' | 'json_object' };
  seed?: number;

  // Metadata
  requestId?: string;
  metadata?: Record<string, any>;
}
```

### Completion Response

```typescript
interface CompletionResponse {
  id: string;
  model: string;
  choices: CompletionChoice[];
  usage: Usage;
  finishReason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'error';
  provider: string;
  latency: number; // milliseconds
  metadata?: Record<string, any>;
}

interface CompletionChoice {
  index: number;
  message: Message;
  finishReason: string;
}

interface Usage {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}
```

### Streaming Response

```typescript
interface StreamChunk {
  id: string;
  model: string;
  choices: StreamChoice[];
  provider: string;
  done: boolean;
}

interface StreamChoice {
  index: number;
  delta: {
    role?: string;
    content?: string;
    toolCalls?: ToolCall[];
  };
  finishReason?: string;
}
```

### Embeddings Request/Response

```typescript
interface EmbeddingsRequest {
  model: string;
  input: string | string[];
  encodingFormat?: 'float' | 'base64';
  dimensions?: number;
}

interface EmbeddingsResponse {
  object: 'list';
  data: Embedding[];
  model: string;
  usage: {
    promptTokens: number;
    totalTokens: number;
  };
  provider: string;
}

interface Embedding {
  object: 'embedding';
  embedding: number[];
  index: number;
}
```

---

## NEAR AI Integration

### API Endpoint

```
Base URL: https://api.near.ai/v1
```

### Authentication

NEAR AI uses API key authentication via the `Authorization` header:

```http
Authorization: Bearer <NEAR_AI_API_KEY>
```

### Supported Models

| Model | Context | Max Tokens | Capabilities |
|-------|---------|------------|--------------|
| `near/gpt-4-turbo` | 128k | 4096 | Completion, Streaming, Tools, JSON |
| `near/gpt-4` | 8k | 4096 | Completion, Streaming, Tools |
| `near/gpt-3.5-turbo` | 16k | 4096 | Completion, Streaming, Tools |
| `near/claude-3-opus` | 200k | 4096 | Completion, Streaming, Vision |
| `near/claude-3-sonnet` | 200k | 4096 | Completion, Streaming, Vision |
| `near/embeddings-v1` | 8k | - | Embeddings |

### API Endpoints

#### Chat Completions

```http
POST /chat/completions
Content-Type: application/json
Authorization: Bearer <API_KEY>

{
  "model": "near/gpt-4-turbo",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Hello!" }
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": false
}
```

#### Streaming Completions

```http
POST /chat/completions
Content-Type: application/json
Authorization: Bearer <API_KEY>

{
  "model": "near/gpt-4-turbo",
  "messages": [...],
  "stream": true
}
```

Response: Server-Sent Events (SSE) stream

#### Embeddings

```http
POST /embeddings
Content-Type: application/json
Authorization: Bearer <API_KEY>

{
  "model": "near/embeddings-v1",
  "input": "Your text here",
  "encoding_format": "float"
}
```

### Rate Limits

| Tier | Requests/Minute | Tokens/Minute | Daily Limit |
|------|-----------------|---------------|-------------|
| Free | 60 | 90,000 | 150,000 |
| Pro | 3,000 | 2,000,000 | 10,000,000 |
| Enterprise | Custom | Custom | Custom |

### Error Codes

| Code | Description | Retryable |
|------|-------------|-----------|
| 400 | Invalid request | No |
| 401 | Invalid API key | No |
| 429 | Rate limit exceeded | Yes |
| 500 | Internal server error | Yes |
| 503 | Service unavailable | Yes |

---

## OpenRouter Integration

### API Endpoint

```
Base URL: https://openrouter.ai/api/v1
```

### Authentication

OpenRouter uses API key authentication:

```http
Authorization: Bearer <OPENROUTER_API_KEY>
```

### Supported Models

OpenRouter provides access to 100+ models from multiple providers. Key models:

| Model | Provider | Context | Max Tokens |
|-------|----------|---------|------------|
| `anthropic/claude-3-opus` | Anthropic | 200k | 4096 |
| `anthropic/claude-3-sonnet` | Anthropic | 200k | 4096 |
| `openai/gpt-4-turbo` | OpenAI | 128k | 4096 |
| `openai/gpt-3.5-turbo` | OpenAI | 16k | 4096 |
| `google/gemini-pro` | Google | 1M | 8192 |
| `meta-llama/llama-3-70b` | Meta | 8k | 4096 |
| `mistralai/mistral-large` | Mistral | 32k | 4096 |

### API Endpoints

#### Chat Completions

```http
POST /chat/completions
Content-Type: application/json
Authorization: Bearer <API_KEY>
HTTP-Referer: <YOUR_SITE_URL>
X-Title: <YOUR_APP_NAME>

{
  "model": "anthropic/claude-3-opus",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Hello!" }
  ],
  "temperature": 0.7,
  "max_tokens": 1000
}
```

**Required Headers:**
- `HTTP-Referer`: Your application URL
- `X-Title`: Your application name

#### Streaming Completions

```http
POST /chat/completions
Content-Type: application/json
Authorization: Bearer <API_KEY>

{
  "model": "anthropic/claude-3-opus",
  "messages": [...],
  "stream": true
}
```

#### Model List

```http
GET /models
Authorization: Bearer <API_KEY>
```

### Pricing

OpenRouter uses a credit-based system. Pricing varies by model:

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| Claude 3 Opus | $15.00 | $75.00 |
| Claude 3 Sonnet | $3.00 | $15.00 |
| GPT-4 Turbo | $10.00 | $30.00 |
| GPT-3.5 Turbo | $0.50 | $1.50 |
| Gemini Pro | $0.50 | $1.50 |
| Llama 3 70B | $0.70 | $0.70 |

### Rate Limits

| Plan | Requests/Minute | Concurrent Requests |
|-------|-----------------|---------------------|
| Free | 20 | 5 |
| Paid | 200 | 50 |
| Enterprise | Custom | Custom |

### Error Codes

| Code | Description | Retryable |
|------|-------------|-----------|
| 400 | Invalid request | No |
| 401 | Invalid API key | No |
| 402 | Insufficient credits | No |
| 429 | Rate limit exceeded | Yes |
| 500 | Provider error | Yes |

---

## API Key Management

### Kernel Keyring Storage

API keys are securely stored using the OS kernel keyring mechanism:

#### Linux (keyutils)

```bash
# Store API key
keyctl add user clawos:near_ai_key "<API_KEY>" @u

# Retrieve API key
keyctl request user clawos:near_ai_key @u

# Delete API key
keyctl purge user clawos:near_ai_key
```

#### macOS (Keychain)

```bash
# Store API key
security add-generic-password \
  -a "clawos" \
  -s "clawos:near_ai_key" \
  -w "<API_KEY>" \
  -U

# Retrieve API key
security find-generic-password \
  -a "clawos" \
  -s "clawos:near_ai_key" \
  -w

# Delete API key
security delete-generic-password \
  -a "clawos" \
  -s "clawos:near_ai_key"
```

#### Windows (Credential Manager)

```powershell
# Store API key
cmdkey /generic:clawos:near_ai_key /user:clawos /pass:<API_KEY>

# Retrieve API key
cmdkey /generic:clawos:near_ai_key

# Delete API key
cmdkey /delete:clawos:near_ai_key
```

### Keyring Interface

```typescript
interface KeyringManager {
  // Store API key
  setKey(provider: string, key: string): Promise<void>;

  // Retrieve API key
  getKey(provider: string): Promise<string | null>;

  // Check if key exists
  hasKey(provider: string): Promise<boolean>;

  // Delete API key
  deleteKey(provider: string): Promise<void>;

  // List all stored keys
  listKeys(): Promise<string[]>;
}
```

### Key Naming Convention

```
clawos:<provider>_key

Examples:
- clawos:near_ai_key
- clawos:openrouter_key
- clawos:anthropic_key
```

### Environment Variable Fallback

If keyring is unavailable, fall back to environment variables:

```bash
export NEAR_AI_API_KEY="your-key-here"
export OPENROUTER_API_KEY="your-key-here"
```

### Security Best Practices

1. **Never log API keys** - Redact from all logs and error messages
2. **Use least privilege** - Generate keys with minimal required scopes
3. **Rotate keys regularly** - Implement key rotation policies
4. **Audit key access** - Log key retrieval operations (without exposing the key)
5. **Encrypt at rest** - Keyring provides OS-level encryption

---

## Fallback Logic

### Fallback Strategy

The bridge implements a hierarchical fallback mechanism:

```
Primary Provider
    │
    ├─► Success → Return response
    │
    └─► Failure
         │
         ├─► Transient error? → Retry (exponential backoff)
         │
         └─► Permanent error / Max retries exceeded
              │
              └─► Fallback to Secondary Provider
                   │
                   ├─► Success → Return response
                   │
                   └─► Failure → Fallback to Tertiary Provider
                        │
                        └─► All failed → Return aggregated error
```

### Fallback Configuration

```typescript
interface FallbackConfig {
  enabled: boolean;
  maxRetries: number;
  retryDelay: number; // milliseconds
  backoffMultiplier: number;
  providers: ProviderPriority[];
}

interface ProviderPriority {
  name: string;
  priority: number; // 1 = highest
  models: string[]; // Models to use from this provider
  conditions?: FallbackCondition[];
}

interface FallbackCondition {
  errorType: 'rate_limit' | 'timeout' | 'server_error' | 'quota_exceeded';
  action: 'retry' | 'fallback' | 'fail';
}
```

### Retry Logic

```typescript
interface RetryPolicy {
  maxAttempts: number;
  initialDelay: number; // milliseconds
  maxDelay: number; // milliseconds
  backoffMultiplier: number;
  jitter: boolean; // Add randomness to prevent thundering herd

  // Retry on specific error codes
  retryableErrors: number[]; // [429, 500, 502, 503, 504]

  // Retry on specific error types
  retryableErrorTypes: string[]; // ['timeout', 'network', 'rate_limit']
}
```

### Example Fallback Configuration

```yaml
fallback:
  enabled: true
  maxRetries: 3
  retryDelay: 1000
  backoffMultiplier: 2

  providers:
    - name: near_ai
      priority: 1
      models:
        - near/gpt-4-turbo
        - near/claude-3-opus
      conditions:
        - errorType: rate_limit
          action: retry
        - errorType: quota_exceeded
          action: fallback

    - name: openrouter
      priority: 2
      models:
        - anthropic/claude-3-opus
        - openai/gpt-4-turbo
      conditions:
        - errorType: rate_limit
          action: retry
        - errorType: server_error
          action: fallback

    - name: local
      priority: 3
      models:
        - local/llama-3-70b
      conditions:
        - errorType: rate_limit
          action: fail
```

### Model Mapping Across Providers

When falling back, map models to equivalent alternatives:

```typescript
const MODEL_FALLBACK_MAP: Record<string, string[]> = {
  'near/gpt-4-turbo': [
    'openai/gpt-4-turbo',
    'anthropic/claude-3-sonnet',
    'google/gemini-pro'
  ],
  'near/claude-3-opus': [
    'anthropic/claude-3-opus',
    'openai/gpt-4-turbo'
  ],
  'near/embeddings-v1': [
    'openai/text-embedding-3-large',
    'cohere/embed-english-v3.0'
  ]
};
```

---

## Rate Limiting & Quotas

### Token Bucket Algorithm

The bridge implements a token bucket rate limiter:

```typescript
interface TokenBucket {
  capacity: number; // Maximum tokens
  tokens: number; // Current tokens
  refillRate: number; // Tokens per second
  lastRefill: number; // Timestamp

  // Consume tokens
  consume(tokens: number): boolean;

  // Refill tokens
  refill(): void;

  // Wait until tokens available
  waitFor(tokens: number): Promise<void>;
}
```

### Rate Limiting Strategy

#### Per-Provider Limits

```typescript
interface ProviderRateLimit {
  requestsPerMinute: number;
  tokensPerMinute: number;
  concurrentRequests: number;
}
```

#### Global Limits

```typescript
interface GlobalRateLimit {
  totalRequestsPerMinute: number;
  totalTokensPerMinute: number;
  maxConcurrentRequests: number;
}
```

#### Per-User Limits (if applicable)

```typescript
interface UserRateLimit {
  userId: string;
  requestsPerHour: number;
  tokensPerDay: number;
}
```

### Quota Management

```typescript
interface QuotaManager {
  // Check if request is within quota
  checkQuota(provider: string, tokens: number): Promise<boolean>;

  // Record usage
  recordUsage(provider: string, tokens: number): Promise<void>;

  // Get remaining quota
  getRemainingQuota(provider: string): Promise<QuotaStatus>;

  // Reset quota (daily/hourly)
  resetQuota(provider: string): Promise<void>;
}

interface QuotaStatus {
  provider: string;
  period: 'hourly' | 'daily' | 'monthly';
  used: number;
  limit: number;
  remaining: number;
  resetAt: Date;
}
```

### Adaptive Rate Limiting

When approaching quota limits, automatically throttle:

```typescript
interface AdaptiveThrottling {
  // Thresholds (percentage of quota)
  warningThreshold: number; // 80%
  criticalThreshold: number; // 95%

  // Throttling factors
  warningFactor: number; // 0.5 (50% speed)
  criticalFactor: number; // 0.1 (10% speed)

  // Apply throttling
  applyThrottling(quotaStatus: QuotaStatus): number;
}
```

### Example Rate Limit Configuration

```yaml
rateLimiting:
  enabled: true

  global:
    requestsPerMinute: 1000
    tokensPerMinute: 10000000
    maxConcurrentRequests: 50

  providers:
    near_ai:
      requestsPerMinute: 3000
      tokensPerMinute: 2000000
      concurrentRequests: 20

    openrouter:
      requestsPerMinute: 200
      tokensPerMinute: 500000
      concurrentRequests: 10

  adaptiveThrottling:
    warningThreshold: 80
    criticalThreshold: 95
    warningFactor: 0.5
    criticalFactor: 0.1
```

---

## Configuration Schema

### Main Configuration File

Location: `config/llm-providers.yaml`

```yaml
# LLM Provider Bridge Configuration
version: "1.0"

# Default provider selection
defaults:
  provider: near_ai
  model: near/gpt-4-turbo
  temperature: 0.7
  maxTokens: 4096

# Provider configurations
providers:
  near_ai:
    enabled: true
    priority: 1
    apiKeySource: keyring # keyring | env | file
    apiKeyEnvVar: NEAR_AI_API_KEY
    baseUrl: https://api.near.ai/v1
    timeout: 30000 # milliseconds
    models:
      - id: near/gpt-4-turbo
        contextLength: 128000
        maxTokens: 4096
        capabilities:
          - completion
          - streaming
          - tools
          - jsonMode
      - id: near/claude-3-opus
        contextLength: 200000
        maxTokens: 4096
        capabilities:
          - completion
          - streaming
          - vision
      - id: near/embeddings-v1
        contextLength: 8000
        capabilities:
          - embeddings

  openrouter:
    enabled: true
    priority: 2
    apiKeySource: keyring
    apiKeyEnvVar: OPENROUTER_API_KEY
    baseUrl: https://openrouter.ai/api/v1
    timeout: 30000
    httpReferer: https://clawos.dev
    appTitle: ClawOS
    models:
      - id: anthropic/claude-3-opus
        contextLength: 200000
        maxTokens: 4096
        capabilities:
          - completion
          - streaming
          - vision
      - id: openai/gpt-4-turbo
        contextLength: 128000
        maxTokens: 4096
        capabilities:
          - completion
          - streaming
          - tools
          - jsonMode

# Fallback configuration
fallback:
  enabled: true
  maxRetries: 3
  retryDelay: 1000
  backoffMultiplier: 2
  strategy: priority # priority | round-robin | random

# Rate limiting
rateLimiting:
  enabled: true
  global:
    requestsPerMinute: 1000
    tokensPerMinute: 10000000
    maxConcurrentRequests: 50
  adaptiveThrottling:
    warningThreshold: 80
    criticalThreshold: 95

# Monitoring
monitoring:
  enabled: true
  logLevel: info # debug | info | warn | error
  metrics:
    - requestCount
    - latency
    - tokenUsage
    - errorRate
    - quotaUsage
```

### Environment Variables

```bash
# NEAR AI
NEAR_AI_API_KEY=your_near_ai_key
NEAR_AI_BASE_URL=https://api.near.ai/v1

# OpenRouter
OPENROUTER_API_KEY=your_openrouter_key
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_HTTP_REFERER=https://clawos.dev
OPENROUTER_APP_TITLE=ClawOS

# General
LLM_DEFAULT_PROVIDER=near_ai
LLM_DEFAULT_MODEL=near/gpt-4-turbo
LLM_TIMEOUT=30000
LLM_MAX_RETRIES=3

# Rate Limiting
LLM_RATE_LIMIT_ENABLED=true
LLM_GLOBAL_RPM=1000
LLM_GLOBAL_TPM=10000000
```

---

## Example Configurations

### Minimal Configuration

```yaml
version: "1.0"

defaults:
  provider: near_ai
  model: near/gpt-4-turbo

providers:
  near_ai:
    enabled: true
    apiKeySource: keyring
```

### Production Configuration

```yaml
version: "1.0"

defaults:
  provider: near_ai
  model: near/gpt-4-turbo
  temperature: 0.7
  maxTokens: 4096

providers:
  near_ai:
    enabled: true
    priority: 1
    apiKeySource: keyring
    baseUrl: https://api.near.ai/v1
    timeout: 30000
    rateLimit:
      requestsPerMinute: 3000
      tokensPerMinute: 2000000

  openrouter:
    enabled: true
    priority: 2
    apiKeySource: keyring
    baseUrl: https://openrouter.ai/api/v1
    timeout: 30000
    httpReferer: https://clawos.dev
    appTitle: ClawOS
    rateLimit:
      requestsPerMinute: 200
      tokensPerMinute: 500000

fallback:
  enabled: true
  maxRetries: 3
  retryDelay: 1000
  backoffMultiplier: 2

rateLimiting:
  enabled: true
  global:
    requestsPerMinute: 1000
    tokensPerMinute: 10000000
    maxConcurrentRequests: 50

monitoring:
  enabled: true
  logLevel: info
```

### Development Configuration

```yaml
version: "1.0"

defaults:
  provider: openrouter
  model: openai/gpt-3.5-turbo
  temperature: 0.7
  maxTokens: 2048

providers:
  openrouter:
    enabled: true
    priority: 1
    apiKeySource: env
    apiKeyEnvVar: OPENROUTER_API_KEY
    baseUrl: https://openrouter.ai/api/v1
    timeout: 60000

fallback:
  enabled: false

rateLimiting:
  enabled: false

monitoring:
  enabled: true
  logLevel: debug
```

### High-Throughput Configuration

```yaml
version: "1.0"

defaults:
  provider: near_ai
  model: near/gpt-4-turbo
  temperature: 0.7
  maxTokens: 4096

providers:
  near_ai:
    enabled: true
    priority: 1
    apiKeySource: keyring
    baseUrl: https://api.near.ai/v1
    timeout: 30000
    rateLimit:
      requestsPerMinute: 3000
      tokensPerMinute: 2000000
      concurrentRequests: 20

  openrouter:
    enabled: true
    priority: 2
    apiKeySource: keyring
    baseUrl: https://openrouter.ai/api/v1
    timeout: 30000
    rateLimit:
      requestsPerMinute: 200
      tokensPerMinute: 500000
      concurrentRequests: 10

fallback:
  enabled: true
  maxRetries: 5
  retryDelay: 500
  backoffMultiplier: 1.5

rateLimiting:
  enabled: true
  global:
    requestsPerMinute: 5000
    tokensPerMinute: 50000000
    maxConcurrentRequests: 100
  adaptiveThrottling:
    warningThreshold: 70
    criticalThreshold: 90
    warningFactor: 0.7
    criticalFactor: 0.3

monitoring:
  enabled: true
  logLevel: warn
```

---

## Error Handling

### Error Types

```typescript
enum ErrorType {
  // Authentication errors
  INVALID_API_KEY = 'INVALID_API_KEY',
  INSUFFICIENT_PERMISSIONS = 'INSUFFICIENT_PERMISSIONS',

  // Rate limiting
  RATE_LIMIT_EXCEEDED = 'RATE_LIMIT_EXCEEDED',
  QUOTA_EXCEEDED = 'QUOTA_EXCEEDED',

  // Request errors
  INVALID_REQUEST = 'INVALID_REQUEST',
  MODEL_NOT_FOUND = 'MODEL_NOT_FOUND',
  CONTEXT_LENGTH_EXCEEDED = 'CONTEXT_LENGTH_EXCEEDED',

  // Network errors
  TIMEOUT = 'TIMEOUT',
  NETWORK_ERROR = 'NETWORK_ERROR',
  CONNECTION_REFUSED = 'CONNECTION_REFUSED',

  // Provider errors
  PROVIDER_UNAVAILABLE = 'PROVIDER_UNAVAILABLE',
  PROVIDER_ERROR = 'PROVIDER_ERROR',

  // Bridge errors
  NO_AVAILABLE_PROVIDER = 'NO_AVAILABLE_PROVIDER',
  ALL_PROVIDERS_FAILED = 'ALL_PROVIDERS_FAILED',
  CONFIGURATION_ERROR = 'CONFIGURATION_ERROR'
}
```

### Error Response Format

```typescript
interface LLMProviderError {
  type: ErrorType;
  message: string;
  provider?: string;
  statusCode?: number;
  requestId?: string;
  retryable: boolean;
  retryAfter?: number; // seconds
  details?: Record<string, any>;
  timestamp: Date;
}
```

### Error Handling Strategy

```typescript
interface ErrorHandler {
  // Determine if error is retryable
  isRetryable(error: LLMProviderError): boolean;

  // Calculate retry delay
  getRetryDelay(error: LLMProviderError, attempt: number): number;

  // Transform provider error to standard format
  normalizeError(providerError: any, provider: string): LLMProviderError;

  // Aggregate multiple provider errors
  aggregateErrors(errors: LLMProviderError[]): LLMProviderError;
}
```

### Retry Logic

```typescript
interface RetryConfig {
  maxAttempts: number;
  initialDelay: number;
  maxDelay: number;
  backoffMultiplier: number;
  jitter: boolean;
  retryableErrors: ErrorType[];
}

async function withRetry<T>(
  fn: () => Promise<T>,
  config: RetryConfig
): Promise<T> {
  let lastError: LLMProviderError;

  for (let attempt = 1; attempt <= config.maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = normalizeError(error);

      if (!isRetryable(lastError) || attempt === config.maxAttempts) {
        throw lastError;
      }

      const delay = calculateRetryDelay(config, attempt);
      await sleep(delay);
    }
  }

  throw lastError;
}
```

### Circuit Breaker Pattern

```typescript
interface CircuitBreaker {
  state: 'closed' | 'open' | 'half-open';
  failureThreshold: number;
  successThreshold: number;
  timeout: number; // milliseconds
  failureCount: number;
  lastFailureTime: number;

  // Execute with circuit breaker protection
  execute<T>(fn: () => Promise<T>): Promise<T>;

  // Reset circuit breaker
  reset(): void;
}
```

---

## Appendix

### A. Model Comparison Matrix

| Feature | NEAR AI | OpenRouter |
|---------|---------|------------|
| Model Variety | 5 core models | 100+ models |
| Streaming | ✅ | ✅ |
| Embeddings | ✅ | ✅ (via providers) |
| Vision | ✅ | ✅ |
| Tools | ✅ | ✅ |
| JSON Mode | ✅ | ✅ |
| Rate Limiting | ✅ | ✅ |
| Fallback | ✅ | ✅ |
| Pricing | Tiered | Per-model credits |

### B. Quick Start Commands

```bash
# Store NEAR AI API key (Linux)
keyctl add user clawos:near_ai_key "your-key" @u

# Store OpenRouter API key (macOS)
security add-generic-password -a "clawos" -s "clawos:openrouter_key" -w "your-key" -U

# Test configuration
clawos llm test --provider near_ai --model near/gpt-4-turbo

# List available models
clawos llm models --provider near_ai

# Check quota status
clawos llm quota --provider near_ai

# View rate limit status
clawos llm rate-limit
```

### C. Configuration Validation

```bash
# Validate configuration file
clawos config validate --file config/llm-providers.yaml

# Test provider connectivity
clawos llm health-check --provider near_ai

# Benchmark provider performance
clawos llm benchmark --provider near_ai --model near/gpt-4-turbo
```

### D. Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| "Invalid API key" | Wrong key stored | Re-store key in keyring |
| "Rate limit exceeded" | Too many requests | Increase rate limit or add fallback |
| "Model not found" | Incorrect model ID | Check available models with `llm models` |
| "Timeout" | Slow network | Increase timeout in config |
| "All providers failed" | Network/API issues | Check connectivity and API status |

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2024-02-24 | Initial specification |

---

## References

- [NEAR AI API Documentation](https://docs.near.ai)
- [OpenRouter API Documentation](https://openrouter.ai/docs)
- [Kernel Keyring (Linux)](https://man7.org/linux/man-pages/man7/keyutils.7.html)
- [macOS Keychain Services](https://developer.apple.com/documentation/security/keychain_services)
- [Windows Credential Manager](https://docs.microsoft.com/en-us/windows/win32/api/wincred/)
