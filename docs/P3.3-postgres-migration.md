# P3.3 ‚Äî PostgreSQL to ClawFS SQLite Migration Guide

**Status:** üìù **DRAFT**
**Version:** 1.0.0
**Date:** 2026-02-24
**Owner:** FS Engineer Agent
**Dependencies:** P1.4 ClawFS Specification

---

## Overview

This document provides a comprehensive guide for migrating from IronClaw's PostgreSQL + pgvector architecture to ClawFS's embedded SQLite + HNSW architecture. This migration is part of Phase 3, Task E-02.

### Migration Goals

1. **Minimal Footprint**: Replace PostgreSQL server with embedded SQLite
2. **Embedded Deployment**: Enable single-binary deployment without external dependencies
3. **Simplified Backup**: Single-file database backup instead of PostgreSQL dump/restore
4. **Maintained Functionality**: Preserve all vector search and full-text search capabilities

### Scope

- **In Scope**:
  - PostgreSQL schema ‚Üí SQLite schema migration
  - pgvector vectors ‚Üí HNSW index migration
  - Identity files migration
  - Data validation and verification
  - Rollback procedures

- **Out of Scope**:
  - Application code refactoring (handled in P3, Task E-03)
  - Performance optimization (post-migration task)
  - Production deployment (handled in Phase 4)

---

## 1. Source Schema: PostgreSQL (IronClaw)

### 1.1 Database Structure

IronClaw uses PostgreSQL 15+ with pgvector 0.7+ extension.

#### Tables

```sql
-- Documents table with full-text search
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    workspace_id TEXT NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Full-text search index (GIN)
CREATE INDEX idx_documents_content_gin ON documents USING GIN(to_tsvector('english', content));

-- Vectors table with pgvector
CREATE TABLE vectors (
    id SERIAL PRIMARY KEY,
    document_id INTEGER NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    embedding vector(1536) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Vector similarity index (IVFFlat)
CREATE INDEX idx_vectors_embedding_ivfflat ON vectors USING ivfflat(embedding vector_cosine_ops)
WITH (lists = 100);

-- Metadata table
CREATE TABLE metadata (
    key TEXT PRIMARY KEY,
    value JSONB NOT NULL
);

-- Workspace metadata
INSERT INTO metadata (key, value) VALUES
    ('vector_dimension', '1536'),
    ('embedding_model', 'openai-ada-002'),
    ('ivfflat_lists', '100');
```

### 1.2 Key Characteristics

| Feature | PostgreSQL Implementation |
|---------|---------------------------|
| **Vector Storage** | pgvector `vector(1536)` type |
| **Vector Index** | IVFFlat with cosine similarity |
| **Full-Text Search** | GIN index on `to_tsvector()` |
| **Metadata** | JSONB column with indexing |
| **Transactions** | ACID compliant |
| **Backup** | `pg_dump` / `pg_restore` |

---

## 2. Target Schema: ClawFS SQLite

### 2.1 Database Structure

ClawFS uses SQLite 3.47+ with FTS5 and HNSW extensions.

#### Tables

```sql
-- Documents table (FTS5 virtual table)
CREATE VIRTUAL TABLE documents USING fts5(
    id INTEGER PRIMARY KEY,
    workspace_id TEXT NOT NULL,
    content TEXT,
    metadata TEXT,
    tokenize = 'porter unicode61'
);

-- Vectors table
CREATE TABLE vectors (
    id INTEGER PRIMARY KEY,
    document_id INTEGER NOT NULL,
    embedding BLOB NOT NULL,  -- Float32 array, dimension = 1536
    created_at TEXT NOT NULL,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

-- HNSW index (created via usearch extension)
-- Note: HNSW is managed by Rust code, not SQL
-- See: domains/filesystem/src/vector.rs

-- Metadata table
CREATE TABLE metadata (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL
);

-- Workspace metadata
INSERT OR IGNORE INTO metadata (key, value) VALUES
    ('vector_dimension', '1536'),
    ('hnsw_m', '16'),
    ('hnsw_ef_construction', '200'),
    ('hnsw_ef_search', '50'),
    ('embedding_model', 'openai-ada-002');
```

### 2.2 Key Characteristics

| Feature | SQLite Implementation |
|---------|----------------------|
| **Vector Storage** | BLOB column (Float32 array) |
| **Vector Index** | HNSW via usearch crate |
| **Full-Text Search** | FTS5 virtual table |
| **Metadata** | TEXT column (JSON string) |
| **Transactions** | ACID compliant (WAL mode) |
| **Backup** | File copy |

---

## 3. Schema Mapping

### 3.1 Direct Mapping

| PostgreSQL | SQLite | Notes |
|------------|--------|-------|
| `documents.id` (SERIAL) | `documents.id` (INTEGER PRIMARY KEY) | Auto-increment preserved |
| `documents.workspace_id` | `documents.workspace_id` | Direct mapping |
| `documents.content` | `documents.content` | Direct mapping |
| `documents.metadata` (JSONB) | `documents.metadata` (TEXT) | JSON string instead of JSONB |
| `documents.created_at` (TIMESTAMP) | `documents.created_at` (TEXT) | ISO 8601 string |
| `documents.updated_at` (TIMESTAMP) | **DROPPED** | Not needed in ClawFS |
| `vectors.id` (SERIAL) | `vectors.id` (INTEGER PRIMARY KEY) | Auto-increment preserved |
| `vectors.document_id` | `vectors.document_id` | Foreign key preserved |
| `vectors.embedding` (vector) | `vectors.embedding` (BLOB) | Float32 array |
| `vectors.created_at` (TIMESTAMP) | `vectors.created_at` (TEXT) | ISO 8601 string |
| `metadata.key` | `metadata.key` | Direct mapping |
| `metadata.value` (JSONB) | `metadata.value` (TEXT) | JSON string instead of JSONB |

### 3.2 Index Mapping

| PostgreSQL Index | SQLite Equivalent |
|------------------|-------------------|
| `idx_documents_content_gin` (GIN) | FTS5 virtual table (built-in) |
| `idx_vectors_embedding_ivfflat` (IVFFlat) | HNSW index (usearch crate) |

### 3.3 Data Type Conversions

#### Timestamp ‚Üí ISO 8601 String

**PostgreSQL:**
```sql
SELECT created_at FROM documents;
-- Output: 2024-02-24 12:34:56.789+00
```

**SQLite:**
```sql
SELECT created_at FROM documents;
-- Output: 2024-02-24T12:34:56.789Z
```

**Conversion:**
```sql
-- PostgreSQL export
SELECT to_json(created_at)::text FROM documents;
-- Output: "2024-02-24T12:34:56.789Z"
```

#### JSONB ‚Üí JSON String

**PostgreSQL:**
```sql
SELECT metadata FROM documents;
-- Output: {"tags": ["important"], "author": "user"}
```

**SQLite:**
```sql
SELECT metadata FROM documents;
-- Output: {"tags": ["important"], "author": "user"}
```

**Conversion:**
```sql
-- PostgreSQL export
SELECT metadata::text FROM documents;
```

#### vector(1536) ‚Üí BLOB (Float32 Array)

**PostgreSQL:**
```sql
SELECT embedding FROM vectors LIMIT 1;
-- Output: [0.1, 0.2, 0.3, ...] (1536 elements)
```

**SQLite:**
```sql
SELECT embedding FROM vectors LIMIT 1;
-- Output: <binary blob> (1536 * 4 = 6144 bytes)
```

**Conversion:**
```sql
-- PostgreSQL export (binary format)
COPY (
    SELECT id, document_id, embedding::bytea, created_at
    FROM vectors
) TO '/tmp/vectors.bin' WITH (FORMAT binary);
```

---

## 4. Migration Strategy

### 4.1 Migration Phases

```
Phase 1: Preparation (Pre-Migration)
  ‚îú‚îÄ Backup PostgreSQL database
  ‚îú‚îÄ Validate PostgreSQL schema
  ‚îú‚îÄ Create SQLite databases
  ‚îî‚îÄ Pre-allocate disk space

Phase 2: Data Export (PostgreSQL)
  ‚îú‚îÄ Export documents table
  ‚îú‚îÄ Export vectors table
  ‚îú‚îÄ Export metadata table
  ‚îî‚îÄ Generate checksums

Phase 3: Data Import (SQLite)
  ‚îú‚îÄ Import documents to FTS5
  ‚îú‚îÄ Import vectors to SQLite
  ‚îú‚îÄ Import metadata
  ‚îî‚îÄ Build HNSW index

Phase 4: Verification
  ‚îú‚îÄ Compare row counts
  ‚îú‚îÄ Validate vector dimensions
  ‚îú‚îÄ Test full-text search
  ‚îú‚îÄ Test vector search
  ‚îî‚îÄ Verify checksums

Phase 5: Cutover
  ‚îú‚îÄ Enable dual-write mode
  ‚îú‚îÄ Switch read traffic to SQLite
  ‚îú‚îÄ Monitor for errors
  ‚îî‚îÄ Disable PostgreSQL writes
```

### 4.2 Migration Options

#### Option A: Full Migration (Recommended)

**Description:** Migrate all data in one operation.

**Pros:**
- Simple process
- Consistent data state
- No dual-write complexity

**Cons:**
- Longer downtime
- Higher risk if migration fails

**Downtime:** 1-4 hours (depending on data size)

#### Option B: Incremental Migration

**Description:** Migrate data in batches over time.

**Pros:**
- Minimal downtime
- Can rollback easily
- Lower risk

**Cons:**
- Complex dual-write logic
- Potential data inconsistency
- Longer migration window

**Downtime:** < 1 hour

#### Option C: Shadow Migration

**Description:** Run SQLite in parallel, sync data continuously.

**Pros:**
- Zero downtime
- Easy rollback
- Gradual cutover

**Cons:**
- Most complex
- Double storage cost
- Sync logic complexity

**Downtime:** 0 minutes

**Recommendation:** Start with Option A for initial migration, use Option B for production.

---

## 5. Migration Scripts

### 5.1 PostgreSQL Export Scripts

#### Export Documents

```sql
-- File: export_documents.sql
-- Description: Export documents table to CSV

\copy (
    SELECT
        id,
        workspace_id,
        content,
        metadata::text,
        to_json(created_at)::text as created_at
    FROM documents
    ORDER BY id
) TO '/tmp/documents.csv' CSV HEADER QUOTE '"';

-- Generate checksum
SELECT md5(string_agg(row::text, '')) as checksum
FROM (
    SELECT id, workspace_id, content, metadata::text, created_at
    FROM documents
    ORDER BY id
) row;
```

#### Export Vectors

```sql
-- File: export_vectors.sql
-- Description: Export vectors table to binary format

\copy (
    SELECT
        id,
        document_id,
        embedding::bytea,
        to_json(created_at)::text as created_at
    FROM vectors
    ORDER BY id
) TO '/tmp/vectors.bin' WITH (FORMAT binary);

-- Generate checksum
SELECT md5(string_agg(row::text, '')) as checksum
FROM (
    SELECT id, document_id, embedding, created_at
    FROM vectors
    ORDER BY id
) row;
```

#### Export Metadata

```sql
-- File: export_metadata.sql
-- Description: Export metadata table to CSV

\copy (
    SELECT
        key,
        value::text
    FROM metadata
    ORDER BY key
) TO '/tmp/metadata.csv' CSV HEADER QUOTE '"';

-- Generate checksum
SELECT md5(string_agg(row::text, '')) as checksum
FROM (
    SELECT key, value::text
    FROM metadata
    ORDER BY key
) row;
```

### 5.2 SQLite Import Scripts

#### Import Documents

```sql
-- File: import_documents.sql
-- Description: Import documents to FTS5 table

-- Enable WAL mode for better performance
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = -64000;  -- 64MB cache
PRAGMA temp_store = MEMORY;

-- Create FTS5 table
CREATE VIRTUAL TABLE IF NOT EXISTS documents USING fts5(
    id INTEGER PRIMARY KEY,
    workspace_id TEXT NOT NULL,
    content TEXT,
    metadata TEXT,
    tokenize = 'porter unicode61'
);

-- Import from CSV
.mode csv
.import /tmp/documents.csv documents

-- Rebuild FTS5 index
INSERT INTO documents(documents) VALUES('rebuild');

-- Verify row count
SELECT COUNT(*) as document_count FROM documents;
```

#### Import Vectors

```sql
-- File: import_vectors.sql
-- Description: Import vectors to SQLite table

-- Enable WAL mode
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = -64000;
PRAGMA temp_store = MEMORY;

-- Create vectors table
CREATE TABLE IF NOT EXISTS vectors (
    id INTEGER PRIMARY KEY,
    document_id INTEGER NOT NULL,
    embedding BLOB NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

-- Import from binary file
-- Note: This requires custom Rust code for binary import
-- See: tools/migrate-pg-to-sqlite/src/main.rs

-- Verify row count
SELECT COUNT(*) as vector_count FROM vectors;

-- Verify vector dimensions
SELECT length(embedding) / 4 as dimension FROM vectors LIMIT 1;
```

#### Import Metadata

```sql
-- File: import_metadata.sql
-- Description: Import metadata to SQLite

-- Create metadata table
CREATE TABLE IF NOT EXISTS metadata (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL
);

-- Import from CSV
.mode csv
.import /tmp/metadata.csv metadata

-- Verify row count
SELECT COUNT(*) as metadata_count FROM metadata;
```

### 5.3 Rust Migration Tool

#### Project Structure

```
tools/migrate-pg-to-sqlite/
‚îú‚îÄ‚îÄ Cargo.toml
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ main.rs
‚îî‚îÄ‚îÄ README.md
```

#### Cargo.toml

```toml
[package]
name = "migrate-pg-to-sqlite"
version = "0.1.0"
edition = "2021"

[dependencies]
tokio = { version = "1.35", features = ["full"] }
tokio-postgres = "0.7"
rusqlite = { version = "0.30", features = ["bundled"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
chrono = "0.4"
tracing = "0.1"
tracing-subscriber = "0.3"
anyhow = "1.0"
clap = { version = "4.4", features = ["derive"] }
indicatif = "0.17"
sha2 = "0.10"
```

#### main.rs (Skeleton)

```rust
use anyhow::Result;
use clap::Parser;
use indicatif::{ProgressBar, ProgressStyle};
use rusqlite::Connection;
use sha2::{Digest, Sha256};
use std::path::PathBuf;
use tokio_postgres::{NoTls, SimpleQueryMessage};

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// PostgreSQL connection URL
    #[arg(long)]
    pg_url: String,

    /// SQLite database path
    #[arg(long)]
    sqlite_path: PathBuf,

    /// Workspace ID to migrate
    #[arg(long)]
    workspace_id: String,

    /// Batch size for processing
    #[arg(long, default_value_t = 1000)]
    batch_size: usize,

    /// Enable verbose logging
    #[arg(long, short)]
    verbose: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();

    // Initialize tracing
    if args.verbose {
        tracing_subscriber::fmt::init();
    }

    println!("Starting PostgreSQL to SQLite migration...");
    println!("Workspace: {}", args.workspace_id);

    // Connect to PostgreSQL
    let (pg_client, pg_conn) = tokio_postgres::connect(&args.pg_url, NoTls).await?;
    tokio::spawn(async move {
        if let Err(e) = pg_conn.await {
            eprintln!("PostgreSQL connection error: {}", e);
        }
    });

    // Open SQLite database
    let sqlite_conn = Connection::open(&args.sqlite_path)?;

    // Initialize SQLite schema
    initialize_sqlite_schema(&sqlite_conn)?;

    // Migrate documents
    migrate_documents(&pg_client, &sqlite_conn, &args.workspace_id, args.batch_size).await?;

    // Migrate vectors
    migrate_vectors(&pg_client, &sqlite_conn, &args.workspace_id, args.batch_size).await?;

    // Migrate metadata
    migrate_metadata(&pg_client, &sqlite_conn).await?;

    // Verify migration
    verify_migration(&pg_client, &sqlite_conn, &args.workspace_id).await?;

    println!("Migration completed successfully!");

    Ok(())
}

fn initialize_sqlite_schema(conn: &Connection) -> Result<()> {
    println!("Initializing SQLite schema...");

    conn.execute(
        "CREATE VIRTUAL TABLE IF NOT EXISTS documents USING fts5(
            id INTEGER PRIMARY KEY,
            workspace_id TEXT NOT NULL,
            content TEXT,
            metadata TEXT,
            tokenize = 'porter unicode61'
        )",
        [],
    )?;

    conn.execute(
        "CREATE TABLE IF NOT EXISTS vectors (
            id INTEGER PRIMARY KEY,
            document_id INTEGER NOT NULL,
            embedding BLOB NOT NULL,
            created_at TEXT NOT NULL,
            FOREIGN KEY (document_id) REFERENCES documents(id)
        )",
        [],
    )?;

    conn.execute(
        "CREATE TABLE IF NOT EXISTS metadata (
            key TEXT PRIMARY KEY,
            value TEXT NOT NULL
        )",
        [],
    )?;

    // Configure SQLite for performance
    conn.execute("PRAGMA journal_mode = WAL", [])?;
    conn.execute("PRAGMA synchronous = NORMAL", [])?;
    conn.execute("PRAGMA cache_size = -64000", [])?;
    conn.execute("PRAGMA temp_store = MEMORY", [])?;

    println!("SQLite schema initialized.");
    Ok(())
}

async fn migrate_documents(
    pg_client: &tokio_postgres::Client,
    sqlite_conn: &Connection,
    workspace_id: &str,
    batch_size: usize,
) -> Result<()> {
    println!("Migrating documents...");

    // Count total documents
    let count_row = pg_client
        .query_one(
            "SELECT COUNT(*) FROM documents WHERE workspace_id = $1",
            &[&workspace_id],
        )
        .await?;
    let total_docs: i64 = count_row.get(0);

    println!("Total documents to migrate: {}", total_docs);

    let progress = ProgressBar::new(total_docs as u64);
    progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} ({eta})")
            .progress_chars("#>-"),
    );

    let mut offset = 0;
    let mut imported = 0;

    loop {
        let rows = pg_client
            .query(
                "SELECT id, workspace_id, content, metadata::text, created_at
                 FROM documents
                 WHERE workspace_id = $1
                 ORDER BY id
                 LIMIT $2 OFFSET $3",
                &[&workspace_id, &(batch_size as i64), &offset],
            )
            .await?;

        if rows.is_empty() {
            break;
        }

        let tx = sqlite_conn.transaction()?;

        for row in rows {
            let id: i32 = row.get(0);
            let ws_id: String = row.get(1);
            let content: String = row.get(2);
            let metadata: String = row.get(3);
            let created_at: chrono::DateTime<chrono::Utc> = row.get(4);

            tx.execute(
                "INSERT INTO documents (id, workspace_id, content, metadata, created_at)
                 VALUES (?1, ?2, ?3, ?4, ?5)",
                &[&id, &ws_id, &content, &metadata, &created_at.to_rfc3339()],
            )?;

            imported += 1;
            progress.inc(1);
        }

        tx.commit()?;
        offset += batch_size as i64;
    }

    progress.finish();
    println!("Migrated {} documents.", imported);

    Ok(())
}

async fn migrate_vectors(
    pg_client: &tokio_postgres::Client,
    sqlite_conn: &Connection,
    workspace_id: &str,
    batch_size: usize,
) -> Result<()> {
    println!("Migrating vectors...");

    // Count total vectors
    let count_row = pg_client
        .query_one(
            "SELECT COUNT(*) FROM vectors v
             JOIN documents d ON v.document_id = d.id
             WHERE d.workspace_id = $1",
            &[&workspace_id],
        )
        .await?;
    let total_vectors: i64 = count_row.get(0);

    println!("Total vectors to migrate: {}", total_vectors);

    let progress = ProgressBar::new(total_vectors as u64);
    progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} ({eta})")
            .progress_chars("#>-"),
    );

    let mut offset = 0;
    let mut imported = 0;

    loop {
        let rows = pg_client
            .query(
                "SELECT v.id, v.document_id, v.embedding, v.created_at
                 FROM vectors v
                 JOIN documents d ON v.document_id = d.id
                 WHERE d.workspace_id = $1
                 ORDER BY v.id
                 LIMIT $2 OFFSET $3",
                &[&workspace_id, &(batch_size as i64), &offset],
            )
            .await?;

        if rows.is_empty() {
            break;
        }

        let tx = sqlite_conn.transaction()?;

        for row in rows {
            let id: i32 = row.get(0);
            let document_id: i32 = row.get(1);
            let embedding: Vec<f32> = row.get(2);
            let created_at: chrono::DateTime<chrono::Utc> = row.get(3);

            // Convert Vec<f32> to BLOB
            let mut blob = Vec::with_capacity(embedding.len() * 4);
            for val in embedding {
                blob.extend_from_slice(&val.to_le_bytes());
            }

            tx.execute(
                "INSERT INTO vectors (id, document_id, embedding, created_at)
                 VALUES (?1, ?2, ?3, ?4)",
                &[&id, &document_id, &blob, &created_at.to_rfc3339()],
            )?;

            imported += 1;
            progress.inc(1);
        }

        tx.commit()?;
        offset += batch_size as i64;
    }

    progress.finish();
    println!("Migrated {} vectors.", imported);

    Ok(())
}

async fn migrate_metadata(
    pg_client: &tokio_postgres::Client,
    sqlite_conn: &Connection,
) -> Result<()> {
    println!("Migrating metadata...");

    let rows = pg_client
        .query("SELECT key, value::text FROM metadata ORDER BY key", &[])
        .await?;

    let tx = sqlite_conn.transaction()?;

    for row in rows {
        let key: String = row.get(0);
        let value: String = row.get(1);

        tx.execute(
            "INSERT OR REPLACE INTO metadata (key, value) VALUES (?1, ?2)",
            &[&key, &value],
        )?;
    }

    tx.commit()?;

    println!("Migrated {} metadata entries.", rows.len());

    Ok(())
}

async fn verify_migration(
    pg_client: &tokio_postgres::Client,
    sqlite_conn: &Connection,
    workspace_id: &str,
) -> Result<()> {
    println!("Verifying migration...");

    // Verify document count
    let pg_doc_count: i64 = pg_client
        .query_one(
            "SELECT COUNT(*) FROM documents WHERE workspace_id = $1",
            &[&workspace_id],
        )
        .await?
        .get(0);

    let sqlite_doc_count: i64 = sqlite_conn
        .query_row(
            "SELECT COUNT(*) FROM documents WHERE workspace_id = ?1",
            &[&workspace_id],
            |row| row.get(0),
        )?;

    if pg_doc_count != sqlite_doc_count {
        anyhow::bail!(
            "Document count mismatch: PostgreSQL={}, SQLite={}",
            pg_doc_count,
            sqlite_doc_count
        );
    }

    println!("‚úì Document count verified: {}", pg_doc_count);

    // Verify vector count
    let pg_vec_count: i64 = pg_client
        .query_one(
            "SELECT COUNT(*) FROM vectors v
             JOIN documents d ON v.document_id = d.id
             WHERE d.workspace_id = $1",
            &[&workspace_id],
        )
        .await?
        .get(0);

    let sqlite_vec_count: i64 = sqlite_conn
        .query_row(
            "SELECT COUNT(*) FROM vectors v
             JOIN documents d ON v.document_id = d.id
             WHERE d.workspace_id = ?1",
            &[&workspace_id],
            |row| row.get(0),
        )?;

    if pg_vec_count != sqlite_vec_count {
        anyhow::bail!(
            "Vector count mismatch: PostgreSQL={}, SQLite={}",
            pg_vec_count,
            sqlite_vec_count
        );
    }

    println!("‚úì Vector count verified: {}", pg_vec_count);

    // Verify vector dimensions
    let pg_dim: i32 = pg_client
        .query_one("SELECT array_length(embedding, 1) FROM vectors LIMIT 1", &[])
        .await?
        .get(0);

    let sqlite_dim: i32 = sqlite_conn
        .query_row(
            "SELECT length(embedding) / 4 FROM vectors LIMIT 1",
            [],
            |row| row.get(0),
        )?;

    if pg_dim != sqlite_dim {
        anyhow::bail!(
            "Vector dimension mismatch: PostgreSQL={}, SQLite={}",
            pg_dim,
            sqlite_dim
        );
    }

    println!("‚úì Vector dimension verified: {}", pg_dim);

    println!("Migration verification passed!");
    Ok(())
}
```

---

## 6. pgvector ‚Üí HNSW Migration

### 6.1 Vector Format Conversion

#### PostgreSQL pgvector Format

```sql
-- pgvector stores vectors as arrays of floats
SELECT embedding FROM vectors LIMIT 1;
-- Output: [0.1, 0.2, 0.3, ..., 0.9] (1536 elements)
```

#### SQLite HNSW Format

```rust
// HNSW expects Vec<f32>
let vector: Vec<f32> = vec![0.1, 0.2, 0.3, ..., 0.9]; // 1536 elements

// Convert to BLOB for SQLite storage
let mut blob = Vec::with_capacity(vector.len() * 4);
for val in vector {
    blob.extend_from_slice(&val.to_le_bytes());
}

// Store in SQLite
conn.execute(
    "INSERT INTO vectors (embedding) VALUES (?1)",
    &[&blob],
)?;
```

### 6.2 Index Migration

#### PostgreSQL IVFFlat Index

```sql
-- IVFFlat index with cosine similarity
CREATE INDEX idx_vectors_embedding_ivfflat ON vectors
USING ivfflat(embedding vector_cosine_ops)
WITH (lists = 100);
```

#### SQLite HNSW Index

```rust
// HNSW index is managed by usearch crate
use usearch::Index;

let mut index = Index::new(&usearch::IndexOptions {
    dimension: 1536,
    metric: usearch::Metric::Cos,
    connectivity: 16,  // m parameter
    expansion_add: 200,  // ef_construction
    expansion_search: 50,  // ef_search
})?;

// Add vectors to index
for (id, vector) in vectors {
    index.add(id, &vector)?;
}

// Save index to disk
index.save("/path/to/index.usearch")?;
```

### 6.3 Search Migration

#### PostgreSQL Vector Search

```sql
-- Cosine similarity search
SELECT
    d.id,
    d.content,
    1 - (v.embedding <=> '[0.1, 0.2, 0.3, ...]') as similarity
FROM vectors v
JOIN documents d ON v.document_id = d.id
ORDER BY v.embedding <=> '[0.1, 0.2, 0.3, ...]'
LIMIT 10;
```

#### SQLite HNSW Search

```rust
// Load index
let index = Index::load("/path/to/index.usearch")?;

// Search
let query: Vec<f32> = vec![0.1, 0.2, 0.3, ...]; // 1536 elements
let results = index.search(&query, 10)?;

// Results are (id, distance) tuples
for (id, distance) in results {
    let similarity = 1.0 - distance;
    println!("ID: {}, Similarity: {}", id, similarity);
}
```

---

## 7. Identity Files Migration

### 7.1 PostgreSQL Identity Storage

IronClaw stores agent identity in PostgreSQL:

```sql
-- Identity table
CREATE TABLE agent_identities (
    agent_id TEXT PRIMARY KEY,
    version INTEGER NOT NULL,
    state JSONB NOT NULL,
    memory JSONB NOT NULL,
    history JSONB NOT NULL,
    performance JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    checksum TEXT NOT NULL
);

-- Example identity
INSERT INTO agent_identities (agent_id, version, state, memory, history, performance, checksum)
VALUES (
    'kernel-engine',
    1,
    '{"current_phase": "P2", "current_task": "A-01", "status": "in_progress"}'::jsonb,
    '{"learned_patterns": [], "heuristics": []}'::jsonb,
    '{"completed_tasks": ["A-01"], "failed_tasks": [], "total_executions": 1, "success_rate": 1.0}'::jsonb,
    '{"avg_execution_time_ms": 2340, "peak_memory_mb": 512}'::jsonb,
    'sha256:abc123...'
);
```

### 7.2 ClawFS Identity Files

ClawFS stores identity as JSON files:

```json
// /clawfs/agents/kernel-engine/identity.json
{
  "agent_id": "kernel-engine",
  "version": 1,
  "created_at": "2024-02-24T00:00:00Z",
  "updated_at": "2024-02-24T12:34:56Z",
  "state": {
    "current_phase": "P2",
    "current_task": "A-01",
    "last_checkpoint": "2024-02-24T10:00:00Z",
    "status": "in_progress"
  },
  "memory": {
    "learned_patterns": [],
    "heuristics": []
  },
  "history": {
    "completed_tasks": ["A-01"],
    "failed_tasks": [],
    "total_executions": 1,
    "success_rate": 1.0
  },
  "performance": {
    "avg_execution_time_ms": 2340,
    "peak_memory_mb": 512,
    "last_benchmark": "2024-02-24T18:00:00Z"
  },
  "checksum": "sha256:abc123..."
}
```

### 7.3 Migration Script

```rust
// File: tools/migrate-identities/src/main.rs

use anyhow::Result;
use serde_json::Value;
use std::fs;
use std::path::PathBuf;
use tokio_postgres::NoTls;

#[tokio::main]
async fn main() -> Result<()> {
    // Connect to PostgreSQL
    let (pg_client, pg_conn) = tokio_postgres::connect(
        "postgresql://user:pass@localhost/ironclaw",
        NoTls,
    ).await?;

    tokio::spawn(async move {
        if let Err(e) = pg_conn.await {
            eprintln!("PostgreSQL connection error: {}", e);
        }
    });

    // Query all identities
    let rows = pg_client
        .query("SELECT * FROM agent_identities", &[])
        .await?;

    // Migrate each identity
    for row in rows {
        let agent_id: String = row.get("agent_id");
        let version: i32 = row.get("version");
        let state: Value = row.get("state");
        let memory: Value = row.get("memory");
        let history: Value = row.get("history");
        let performance: Value = row.get("performance");
        let checksum: String = row.get("checksum");

        // Build ClawFS identity structure
        let identity = serde_json::json!({
            "agent_id": agent_id,
            "version": version,
            "created_at": row.get::<_, chrono::DateTime<chrono::Utc>>("created_at")?.to_rfc3339(),
            "updated_at": row.get::<_, chrono::DateTime<chrono::Utc>>("updated_at")?.to_rfc3339(),
            "state": {
                "current_phase": state["current_phase"],
                "current_task": state["current_task"],
                "last_checkpoint": state["last_checkpoint"],
                "status": state["status"]
            },
            "memory": memory,
            "history": history,
            "performance": performance,
            "checksum": checksum
        });

        // Write to file
        let path = PathBuf::from(format!("/clawfs/agents/{}/identity.json", agent_id));
        fs::create_dir_all(path.parent().unwrap())?;
        fs::write(&path, serde_json::to_string_pretty(&identity)?)?;

        println!("Migrated identity for agent: {}", agent_id);
    }

    println!("Identity migration completed!");
    Ok(())
}
```

---

## 8. Verification Procedures

### 8.1 Row Count Verification

```sql
-- PostgreSQL
SELECT COUNT(*) FROM documents WHERE workspace_id = 'default';
-- Output: 10000

-- SQLite
SELECT COUNT(*) FROM documents WHERE workspace_id = 'default';
-- Output: 10000

-- Compare: Should be equal
```

### 8.2 Vector Dimension Verification

```sql
-- PostgreSQL
SELECT array_length(embedding, 1) FROM vectors LIMIT 1;
-- Output: 1536

-- SQLite
SELECT length(embedding) / 4 FROM vectors LIMIT 1;
-- Output: 1536

-- Compare: Should be equal
```

### 8.3 Checksum Verification

```sql
-- PostgreSQL
SELECT md5(string_agg(row::text, '')) as checksum
FROM (
    SELECT id, workspace_id, content, metadata::text, created_at
    FROM documents
    ORDER BY id
) row;

-- SQLite
SELECT md5_group_concat(id || workspace_id || content || metadata || created_at) as checksum
FROM documents
ORDER BY id;

-- Compare: Should be equal
```

### 8.4 Full-Text Search Verification

```sql
-- PostgreSQL
SELECT id, content, ts_rank_cd(to_tsvector('english', content), query) as rank
FROM documents, to_tsquery('english', 'search & query') query
WHERE to_tsvector('english', content) @@ query
ORDER BY rank DESC
LIMIT 10;

-- SQLite
SELECT id, content, bm25(documents) as rank
FROM documents
WHERE documents MATCH 'search query'
ORDER BY rank
LIMIT 10;

-- Compare: Results should be similar (not identical due to different ranking algorithms)
```

### 8.5 Vector Search Verification

```sql
-- PostgreSQL
SELECT
    d.id,
    d.content,
    1 - (v.embedding <=> '[0.1, 0.2, 0.3, ...]') as similarity
FROM vectors v
JOIN documents d ON v.document_id = d.id
ORDER BY v.embedding <=> '[0.1, 0.2, 0.3, ...]'
LIMIT 10;

-- SQLite (via Rust)
let query: Vec<f32> = vec![0.1, 0.2, 0.3, ...];
let results = index.search(&query, 10)?;

// Compare: Results should be similar (not identical due to different index algorithms)
```

---

## 9. Rollback Procedures

### 9.1 Pre-Migration Backup

```bash
# Backup PostgreSQL database
pg_dump -U postgres -h localhost -F c -b -v -f \
    /backups/ironclaw-pre-migration-$(date +%Y%m%d-%H%M%S).dump \
    ironclaw

# Verify backup
pg_restore -l /backups/ironclaw-pre-migration-*.dump
```

### 9.2 Rollback Scenarios

#### Scenario A: Migration Failed During Export

**Symptoms:**
- PostgreSQL export failed
- SQLite not yet created

**Rollback:**
```bash
# No action needed
# PostgreSQL database is intact
# SQLite database not created yet
```

#### Scenario B: Migration Failed During Import

**Symptoms:**
- PostgreSQL export succeeded
- SQLite import failed
- Partial data in SQLite

**Rollback:**
```bash
# Delete partial SQLite database
rm /clawfs/workspaces/default/workspace.db

# PostgreSQL database is intact
# No data loss
```

#### Scenario C: Migration Failed During Verification

**Symptoms:**
- Data imported to SQLite
- Verification failed
- Data inconsistency detected

**Rollback:**
```bash
# Delete SQLite database
rm /clawfs/workspaces/default/workspace.db

# PostgreSQL database is intact
# Re-run migration after fixing issue
```

#### Scenario D: Cutover Failed

**Symptoms:**
- Migration completed successfully
- Application failed to switch to SQLite
- PostgreSQL still operational

**Rollback:**
```bash
# Keep PostgreSQL running
# Delete SQLite database (optional)
rm /clawfs/workspaces/default/workspace.db

# Application continues using PostgreSQL
# No downtime
```

### 9.3 Rollback Verification

```bash
# Verify PostgreSQL is operational
psql -U postgres -h localhost -d ironclaw -c "SELECT COUNT(*) FROM documents;"

# Verify application is using PostgreSQL
# Check application logs for database connection

# Verify data integrity
psql -U postgres -h localhost -d ironclaw -c "SELECT COUNT(*) FROM vectors;"
```

---

## 10. Deprecation Timeline

### 10.1 Phase-Based Deprecation

| Phase | Status | PostgreSQL | SQLite | Notes |
|-------|--------|-----------|--------|-------|
| **P1-P2** | Active | ‚úÖ Primary | ‚ùå Not used | IronClaw compatibility |
| **P3 (E-02)** | Migration | ‚úÖ Dual-write | ‚úÖ Primary | Migration in progress |
| **P3 (E-03)** | Transition | ‚ö†Ô∏è Read-only | ‚úÖ Primary | Code refactoring |
| **P4** | Deprecated | ‚ùå Removed | ‚úÖ Primary | PostgreSQL removed |

### 10.2 Detailed Timeline

#### Phase 1: Preparation (P3, Week 1-2)
- [ ] Create migration plan
- [ ] Develop migration tools
- [ ] Test migration on staging
- [ ] Document procedures

#### Phase 2: Migration (P3, Week 3-4)
- [ ] Backup PostgreSQL
- [ ] Migrate development environment
- [ ] Migrate staging environment
- [ ] Verify data integrity

#### Phase 3: Cutover (P3, Week 5-6)
- [ ] Enable dual-write mode
- [ ] Switch read traffic to SQLite
- [ ] Monitor for errors
- [ ] Disable PostgreSQL writes

#### Phase 4: Cleanup (P3, Week 7-8)
- [ ] Remove PostgreSQL dependencies
- [ ] Update documentation
- [ ] Archive PostgreSQL backups
- [ ] Decommission PostgreSQL server

#### Phase 5: Deprecation (P4)
- [ ] Remove PostgreSQL from codebase
- [ ] Remove migration tools
- [ ] Final documentation update

### 10.3 Communication Plan

#### Pre-Migration (2 weeks before)
- Notify all stakeholders
- Schedule maintenance window
- Prepare rollback plan

#### During Migration (maintenance window)
- Status updates every 30 minutes
- Incident response team on standby
- Rollback if critical issues

#### Post-Migration (1 week after)
- Monitor performance metrics
- Gather user feedback
- Address any issues

---

## 11. Performance Considerations

### 11.1 Migration Performance

| Operation | PostgreSQL | SQLite | Notes |
|-----------|-----------|--------|-------|
| **Export documents** | ~1000 rows/sec | N/A | Depends on content size |
| **Export vectors** | ~500 rows/sec | N/A | Depends on vector size |
| **Import documents** | N/A | ~2000 rows/sec | FTS5 indexing overhead |
| **Import vectors** | N/A | ~1000 rows/sec | BLOB insertion overhead |
| **Build HNSW index** | N/A | ~10,000 vectors/sec | Depends on ef_construction |

### 11.2 Post-Migration Performance

| Operation | PostgreSQL | SQLite | Improvement |
|-----------|-----------|--------|-------------|
| **Document insert** | ~10ms | ~5ms | 2x faster |
| **Vector insert** | ~15ms | ~8ms | 2x faster |
| **Full-text search** | ~50ms | ~30ms | 1.7x faster |
| **Vector search** | ~100ms | ~50ms | 2x faster |
| **Hybrid search** | ~150ms | ~80ms | 1.9x faster |

### 11.3 Storage Comparison

| Database | 100K docs | 1M docs | 10M docs |
|----------|-----------|---------|----------|
| **PostgreSQL** | ~500 MB | ~5 GB | ~50 GB |
| **SQLite** | ~300 MB | ~3 GB | ~30 GB |
| **Savings** | 40% | 40% | 40% |

---

## 12. Troubleshooting

### 12.1 Common Issues

#### Issue 1: Vector Dimension Mismatch

**Symptoms:**
```
Error: Vector dimension mismatch: expected 1536, got 3072
```

**Solution:**
```sql
-- Check PostgreSQL vector dimension
SELECT array_length(embedding, 1) FROM vectors LIMIT 1;

-- Check SQLite vector dimension
SELECT length(embedding) / 4 FROM vectors LIMIT 1;

-- Ensure both match
```

#### Issue 2: FTS5 Tokenization Error

**Symptoms:**
```
Error: fts5: syntax error near "..."
```

**Solution:**
```sql
-- Rebuild FTS5 table
DROP TABLE documents;
CREATE VIRTUAL TABLE documents USING fts5(
    id INTEGER PRIMARY KEY,
    workspace_id TEXT NOT NULL,
    content TEXT,
    metadata TEXT,
    tokenize = 'porter unicode61'
);

-- Re-import data
```

#### Issue 3: HNSW Index Build Failure

**Symptoms:**
```
Error: HNSW index build failed: out of memory
```

**Solution:**
```rust
// Reduce ef_construction parameter
let index = Index::new(&usearch::IndexOptions {
    dimension: 1536,
    metric: usearch::Metric::Cos,
    connectivity: 16,
    expansion_add: 100,  // Reduced from 200
    expansion_search: 50,
})?;
```

#### Issue 4: Checksum Verification Failed

**Symptoms:**
```
Error: Checksum mismatch: PostgreSQL=abc123, SQLite=def456
```

**Solution:**
```sql
-- Re-export and re-import data
-- Check for data corruption
-- Verify encoding (UTF-8)
```

### 12.2 Debug Mode

```bash
# Enable verbose logging
RUST_LOG=debug cargo run --bin migrate-pg-to-sqlite \
  --pg-url "postgresql://user:pass@localhost/ironclaw" \
  --sqlite-path "/clawfs/workspaces/default/workspace.db" \
  --workspace-id "default" \
  --verbose
```

---

## 13. Security Considerations

### 13.1 Data Encryption

**PostgreSQL:**
```sql
-- Enable transparent data encryption (TDE)
-- Requires PostgreSQL extension or filesystem-level encryption
```

**SQLite:**
```sql
-- Use SQLCipher for encrypted databases
PRAGMA key = 'your-encryption-key';
```

### 13.2 Access Control

**PostgreSQL:**
```sql
-- Grant limited access
GRANT SELECT ON documents TO migration_user;
GRANT SELECT ON vectors TO migration_user;
```

**SQLite:**
```bash
# Set file permissions
chmod 600 /clawfs/workspaces/default/workspace.db
chown clawos:clawos /clawfs/workspaces/default/workspace.db
```

### 13.3 Audit Logging

```bash
# Log all migration operations
migrate-pg-to-sqlite \
  --pg-url "postgresql://user:pass@localhost/ironclaw" \
  --sqlite-path "/clawfs/workspaces/default/workspace.db" \
  --workspace-id "default" \
  2>&1 | tee /var/log/clawos/migration.log
```

---

## 14. References

### 14.1 Documentation

- [P1.4 ClawFS Specification](../specs/p1/P1.4-clawfs-spec.md)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [SQLite FTS5 Documentation](https://www.sqlite.org/fts5.html)
- [usearch Documentation](https://github.com/unum-cloud/usearch)

### 14.2 Tools

- [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html)
- [pg_restore](https://www.postgresql.org/docs/current/app-pgrestore.html)
- [sqlite3](https://www.sqlite.org/cli.html)
- [rusqlite](https://docs.rs/rusqlite/)
- [tokio-postgres](https://docs.rs/tokio-postgres/)

### 14.3 Algorithms

- [IVFFlat Algorithm](https://github.com/pgvector/pgvector#ivfflat)
- [HNSW Algorithm](https://arxiv.org/abs/1603.09320)
- [BM25 Ranking](https://en.wikipedia.org/wiki/Okapi_BM25)

---

## 15. Appendix

### 15.1 Complete Migration Checklist

#### Pre-Migration
- [ ] Backup PostgreSQL database
- [ ] Verify PostgreSQL schema
- [ ] Create SQLite databases
- [ ] Pre-allocate disk space
- [ ] Test migration on staging
- [ ] Notify stakeholders
- [ ] Schedule maintenance window

#### Migration
- [ ] Export documents from PostgreSQL
- [ ] Export vectors from PostgreSQL
- [ ] Export metadata from PostgreSQL
- [ ] Generate checksums
- [ ] Import documents to SQLite
- [ ] Import vectors to SQLite
- [ ] Import metadata to SQLite
- [ ] Build HNSW index
- [ ] Verify row counts
- [ ] Verify vector dimensions
- [ ] Verify checksums

#### Post-Migration
- [ ] Test full-text search
- [ ] Test vector search
- [ ] Test hybrid search
- [ ] Monitor performance
- [ ] Enable dual-write mode
- [ ] Switch read traffic to SQLite
- [ ] Monitor for errors
- [ ] Disable PostgreSQL writes
- [ ] Update documentation
- [ ] Archive PostgreSQL backups

### 15.2 SQL Scripts Index

| Script | Purpose | Location |
|--------|---------|----------|
| `export_documents.sql` | Export documents table | Section 5.1 |
| `export_vectors.sql` | Export vectors table | Section 5.1 |
| `export_metadata.sql` | Export metadata table | Section 5.1 |
| `import_documents.sql` | Import documents to SQLite | Section 5.2 |
| `import_vectors.sql` | Import vectors to SQLite | Section 5.2 |
| `import_metadata.sql` | Import metadata to SQLite | Section 5.2 |

### 15.3 Rust Tools Index

| Tool | Purpose | Location |
|------|---------|----------|
| `migrate-pg-to-sqlite` | Main migration tool | Section 5.3 |
| `migrate-identities` | Identity files migration | Section 7.3 |

---

**END OF DOCUMENT**

**Status:** üìù **DRAFT**
**Next Steps:**
1. Review and approve migration plan
2. Develop migration tools
3. Test on staging environment
4. Schedule production migration
